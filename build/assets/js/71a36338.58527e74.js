"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[983],{8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var a=t(6540);const i={},s=a.createContext(i);function o(e){const n=a.useContext(s);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:n},e.children)}},8516:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>u});var a=t(4848),i=t(8453);const s={sidebar_position:3},o="Multimodal Perception: Vision-Language Integration",r={id:"module-4-vla/multimodal-perception",title:"Multimodal Perception: Vision-Language Integration",description:"Learning Objectives",source:"@site/docs/module-4-vla/multimodal-perception.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/multimodal-perception",permalink:"/ai-robotic-book/docs/module-4-vla/multimodal-perception",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/ai-robotic-book/tree/main/docs/module-4-vla/multimodal-perception.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"VLA Architecture: Vision-Language-Action Fundamentals",permalink:"/ai-robotic-book/docs/module-4-vla/vla-architecture"},next:{title:"Language-Guided Manipulation: From Commands to Actions",permalink:"/ai-robotic-book/docs/module-4-vla/language-guided-manipulation"}},l={},u=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Multimodal Perception",id:"introduction-to-multimodal-perception",level:2},{value:"Multimodal Perception Architecture",id:"multimodal-perception-architecture",level:2},{value:"Core Components",id:"core-components",level:3},{value:"1. Visual Processing Pipeline",id:"1-visual-processing-pipeline",level:4},{value:"2. Language Processing Pipeline",id:"2-language-processing-pipeline",level:4},{value:"3. Multimodal Fusion Mechanism",id:"3-multimodal-fusion-mechanism",level:4},{value:"Visual Grounding Systems",id:"visual-grounding-systems",level:2},{value:"Object Detection with Language Conditioning",id:"object-detection-with-language-conditioning",level:3},{value:"Spatial Reasoning and Layout Understanding",id:"spatial-reasoning-and-layout-understanding",level:3},{value:"Scene Understanding with Language",id:"scene-understanding-with-language",level:2},{value:"Semantic Scene Graph Generation",id:"semantic-scene-graph-generation",level:3},{value:"Language-Grounded Scene Understanding",id:"language-grounded-scene-understanding",level:3},{value:"Implementation and Integration",id:"implementation-and-integration",level:2},{value:"Complete Multimodal Perception System",id:"complete-multimodal-perception-system",level:3},{value:"Evaluation and Benchmarking",id:"evaluation-and-benchmarking",level:2},{value:"Multimodal Perception Metrics",id:"multimodal-perception-metrics",level:3},{value:"Summary",id:"summary",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"multimodal-perception-vision-language-integration",children:"Multimodal Perception: Vision-Language Integration"}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand multimodal perception architectures for vision-language fusion"}),"\n",(0,a.jsx)(n.li,{children:"Implement visual grounding and object detection with language conditioning"}),"\n",(0,a.jsx)(n.li,{children:"Create spatial reasoning systems that connect language to visual scenes"}),"\n",(0,a.jsx)(n.li,{children:"Design multimodal feature extraction and fusion mechanisms"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate multimodal perception performance in robotic contexts"}),"\n",(0,a.jsx)(n.li,{children:"Integrate multimodal perception with robotic action systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"introduction-to-multimodal-perception",children:"Introduction to Multimodal Perception"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Multimodal perception"})," in robotics involves the integration of multiple sensory modalities, primarily vision and language, to create a comprehensive understanding of the environment. Unlike traditional perception systems that process each modality independently, multimodal perception systems combine information from different sources to achieve more robust and semantically rich scene understanding."]}),"\n",(0,a.jsx)(n.p,{children:"Multimodal perception enables robots to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand natural language descriptions of visual scenes"}),"\n",(0,a.jsx)(n.li,{children:"Ground language commands to specific visual objects and locations"}),"\n",(0,a.jsx)(n.li,{children:"Perform spatial reasoning based on both visual and linguistic information"}),"\n",(0,a.jsx)(n.li,{children:"Handle ambiguous or incomplete information through cross-modal inference"}),"\n",(0,a.jsx)(n.li,{children:"Interact naturally with humans using both visual and linguistic cues"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"multimodal-perception-architecture",children:"Multimodal Perception Architecture"}),"\n",(0,a.jsx)(n.h3,{id:"core-components",children:"Core Components"}),"\n",(0,a.jsx)(n.h4,{id:"1-visual-processing-pipeline",children:"1. Visual Processing Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# visual_processing.py\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom transformers import CLIPVisionModel\nimport cv2\nimport numpy as np\n\nclass VisualProcessor(nn.Module):\n    def __init__(self, model_name=\"openai/clip-vit-base-patch32\"):\n        super(VisualProcessor, self).__init__()\n\n        # CLIP vision encoder\n        self.clip_vision = CLIPVisionModel.from_pretrained(model_name)\n\n        # Spatial feature extractor\n        self.spatial_encoder = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool2d((7, 7))\n        )\n\n        # Feature fusion layer\n        self.feature_fusion = nn.Sequential(\n            nn.Linear(512 + 128, 512),  # CLIP + spatial features\n            nn.LayerNorm(512),\n            nn.ReLU()\n        )\n\n        # Spatial position encoding\n        self.pos_encoder = nn.Parameter(torch.randn(1, 50, 512))  # 7x7 = 49 + 1 for global\n\n    def forward(self, images):\n        batch_size, channels, height, width = images.shape\n\n        # Process with CLIP vision encoder\n        clip_features = self.clip_vision(pixel_values=images)\n        global_features = clip_features.pooler_output  # (batch, 512)\n        patch_features = clip_features.last_hidden_state  # (batch, num_patches, 768)\n\n        # Extract spatial features\n        spatial_features = self.spatial_encoder(images)  # (batch, 128, 7, 7)\n        spatial_features_flat = spatial_features.view(batch_size, 128, -1).transpose(-1, -2)  # (batch, 49, 128)\n\n        # Fuse visual features\n        # Repeat global features for each spatial location\n        global_repeated = global_features.unsqueeze(1).repeat(1, 49, 1)  # (batch, 49, 512)\n\n        # Concatenate and fuse\n        fused_features = self.feature_fusion(\n            torch.cat([global_repeated, spatial_features_flat], dim=-1)\n        )\n\n        # Add global feature\n        global_feature_expanded = global_features.unsqueeze(1)  # (batch, 1, 512)\n        all_features = torch.cat([global_feature_expanded, fused_features], dim=1)  # (batch, 50, 512)\n\n        # Add positional encoding\n        pos_enc = self.pos_encoder[:, :all_features.size(1), :].expand(batch_size, -1, -1)\n        final_features = all_features + pos_enc\n\n        return {\n            'features': final_features,  # (batch, seq_len, dim)\n            'global_features': global_features,  # (batch, dim)\n            'spatial_features': fused_features  # (batch, 49, 512)\n        }\n"})}),"\n",(0,a.jsx)(n.h4,{id:"2-language-processing-pipeline",children:"2. Language Processing Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# language_processing.py\nimport torch\nimport torch.nn as nn\nfrom transformers import LlamaModel, LlamaTokenizer\nimport re\n\nclass LanguageProcessor(nn.Module):\n    def __init__(self, model_name=\"meta-llama/Llama-2-7b-hf\"):\n        super(LanguageProcessor, self).__init__()\n\n        self.tokenizer = LlamaTokenizer.from_pretrained(model_name)\n        self.llm = LlamaModel.from_pretrained(model_name)\n\n        # Add special tokens for multimodal processing\n        special_tokens = {\n            'additional_special_tokens': [\n                '<OBJECT>', '</OBJECT>',  # For object references\n                '<LOCATION>', '</LOCATION>',  # For spatial references\n                '<ACTION>', '</ACTION>',  # For action descriptions\n                '<VISUAL>', '</VISUAL>'   # For visual context\n            ]\n        }\n        self.tokenizer.add_special_tokens(special_tokens)\n\n        # Projection layer for visual features\n        self.visual_projection = nn.Linear(512, self.llm.config.hidden_size)\n\n        # Attention mechanism for visual grounding\n        self.visual_attention = nn.MultiheadAttention(\n            embed_dim=self.llm.config.hidden_size,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Classification head for grounding\n        self.grounding_head = nn.Linear(self.llm.config.hidden_size, 1)\n\n    def tokenize_text(self, texts, max_length=512):\n        \"\"\"Tokenize text with special handling for multimodal content\"\"\"\n        # Add visual context markers if not present\n        processed_texts = []\n        for text in texts:\n            if '<VISUAL>' not in text:\n                text = f'<VISUAL>{text}</VISUAL>'\n            processed_texts.append(text)\n\n        return self.tokenizer(\n            processed_texts,\n            padding=True,\n            truncation=True,\n            max_length=max_length,\n            return_tensors='pt'\n        )\n\n    def forward(self, input_ids, attention_mask, visual_features=None):\n        # Process language tokens\n        language_outputs = self.llm(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        language_features = language_outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n\n        if visual_features is not None:\n            # Apply cross-attention with visual features\n            visual_projected = self.visual_projection(visual_features)  # (batch, vis_seq_len, hidden_size)\n\n            # Apply attention: language attends to visual features\n            attended_features, attention_weights = self.visual_attention(\n                language_features,  # query\n                visual_projected,   # key\n                visual_projected    # value\n            )\n\n            # Combine original language features with attended visual features\n            combined_features = language_features + attended_features\n\n            # Get grounding scores for each token\n            grounding_scores = self.grounding_head(combined_features)  # (batch, seq_len, 1)\n\n            return {\n                'features': combined_features,\n                'attention_weights': attention_weights,\n                'grounding_scores': grounding_scores,\n                'language_features': language_features\n            }\n\n        return {\n            'features': language_features,\n            'language_features': language_features\n        }\n\n    def extract_entities(self, text, grounding_scores, threshold=0.5):\n        \"\"\"Extract entities and their grounding scores from text\"\"\"\n        tokens = self.tokenizer.tokenize(text)\n\n        entities = []\n        current_entity = \"\"\n        current_score = 0\n        entity_count = 0\n\n        for i, (token, score) in enumerate(zip(tokens, grounding_scores[0])):\n            if score > threshold:\n                if current_entity:\n                    current_entity += \" \" + token\n                else:\n                    current_entity = token\n                    entity_start = i\n                current_score += score.item()\n            else:\n                if current_entity:\n                    entities.append({\n                        'text': current_entity,\n                        'score': current_score / len(current_entity.split()),\n                        'start_idx': entity_start,\n                        'end_idx': i\n                    })\n                    current_entity = \"\"\n                    current_score = 0\n\n        if current_entity:\n            entities.append({\n                'text': current_entity,\n                'score': current_score / len(current_entity.split()),\n                'start_idx': entity_start,\n                'end_idx': len(tokens)\n            })\n\n        return entities\n"})}),"\n",(0,a.jsx)(n.h4,{id:"3-multimodal-fusion-mechanism",children:"3. Multimodal Fusion Mechanism"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# multimodal_fusion.py\nimport torch\nimport torch.nn as nn\n\nclass MultimodalFusion(nn.Module):\n    def __init__(self, visual_dim=512, language_dim=4096, output_dim=512):\n        super(MultimodalFusion, self).__init__()\n\n        self.visual_dim = visual_dim\n        self.language_dim = language_dim\n        self.output_dim = output_dim\n\n        # Projection layers to match dimensions\n        self.visual_project = nn.Linear(visual_dim, output_dim)\n        self.language_project = nn.Linear(language_dim, output_dim)\n\n        # Cross-attention mechanism\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=output_dim,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Self-attention for multimodal features\n        self.self_attention = nn.MultiheadAttention(\n            embed_dim=output_dim,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Feed-forward network\n        self.ffn = nn.Sequential(\n            nn.Linear(output_dim, output_dim * 4),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(output_dim * 4, output_dim)\n        )\n\n        # Layer normalization\n        self.norm1 = nn.LayerNorm(output_dim)\n        self.norm2 = nn.LayerNorm(output_dim)\n\n        # Final output projection\n        self.output_projection = nn.Linear(output_dim, output_dim)\n\n    def forward(self, visual_features, language_features):\n        \"\"\"\n        Fuse visual and language features\n\n        Args:\n            visual_features: (batch, vis_seq_len, visual_dim)\n            language_features: (batch, lang_seq_len, language_dim)\n        \"\"\"\n        batch_size = visual_features.size(0)\n\n        # Project features to common dimension\n        vis_proj = self.visual_project(visual_features)  # (batch, vis_seq_len, output_dim)\n        lang_proj = self.language_project(language_features)  # (batch, lang_seq_len, output_dim)\n\n        # Cross-attention: visual features attend to language features\n        vis_attended, vis_attention = self.cross_attention(\n            vis_proj,  # query\n            lang_proj,  # key\n            lang_proj   # value\n        )\n\n        # Cross-attention: language features attend to visual features\n        lang_attended, lang_attention = self.cross_attention(\n            lang_proj,  # query\n            vis_proj,   # key\n            vis_proj    # value\n        )\n\n        # Concatenate attended features\n        combined_features = torch.cat([\n            vis_attended,\n            lang_attended\n        ], dim=1)  # (batch, vis_seq_len + lang_seq_len, output_dim)\n\n        # Self-attention on combined features\n        attended_combined, self_attention = self.self_attention(\n            combined_features,\n            combined_features,\n            combined_features\n        )\n\n        # Add & Norm\n        norm1_output = self.norm1(combined_features + attended_combined)\n\n        # Feed Forward & Norm\n        ffn_output = self.ffn(norm1_output)\n        multimodal_features = self.norm2(norm1_output + ffn_output)\n\n        # Final projection\n        final_features = self.output_projection(multimodal_features)\n\n        return {\n            'fused_features': final_features,\n            'vis_attention': vis_attention,\n            'lang_attention': lang_attention,\n            'self_attention': self_attention\n        }\n"})}),"\n",(0,a.jsx)(n.h2,{id:"visual-grounding-systems",children:"Visual Grounding Systems"}),"\n",(0,a.jsx)(n.h3,{id:"object-detection-with-language-conditioning",children:"Object Detection with Language Conditioning"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# visual_grounding.py\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom transformers import LlamaModel\n\nclass VisualGroundingSystem(nn.Module):\n    def __init__(self, language_model_name="meta-llama/Llama-2-7b-hf"):\n        super(VisualGroundingSystem, self).__init__()\n\n        # Object detection backbone\n        self.object_detector = fasterrcnn_resnet50_fpn(pretrained=True)\n        detector_features = self.object_detector.backbone.out_channels\n\n        # Language encoder\n        self.language_encoder = LlamaModel.from_pretrained(language_model_name)\n        self.lang_proj = nn.Linear(self.language_encoder.config.hidden_size, 256)\n\n        # Visual feature projection\n        self.vis_proj = nn.Linear(detector_features, 256)\n\n        # Cross-modal attention for grounding\n        self.grounding_attention = nn.MultiheadAttention(\n            embed_dim=256,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Grounding score predictor\n        self.grounding_predictor = nn.Linear(256, 1)\n\n        # Bounding box refinement\n        self.bbox_refinement = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 4)  # dx, dy, dw, dh\n        )\n\n    def forward(self, images, text_queries):\n        """\n        Ground text queries to visual objects\n\n        Args:\n            images: List of images or batched tensor\n            text_queries: List of text queries for each image\n        """\n        batch_size = len(images) if isinstance(images, list) else images.size(0)\n\n        # Extract visual features using object detector\n        if isinstance(images, list):\n            # Convert list to tensor if needed\n            images_tensor = torch.stack([img for img in images])\n        else:\n            images_tensor = images\n\n        # Get visual features from backbone\n        visual_features = self.object_detector.backbone(images_tensor)\n\n        # Extract object proposals\n        proposals = self.object_detector.rpn(images_tensor, visual_features)\n\n        # Get object detections\n        detections = self.object_detector.roi_heads(visual_features, proposals, images_tensor.shape[-2:])\n\n        # Process language queries\n        tokenized_queries = []\n        for query in text_queries:\n            tokens = self.language_encoder.embed_tokens(\n                torch.tensor([self.language_encoder.config.vocab_size-1])  # Simplified\n            )\n            tokenized_queries.append(tokens)\n\n        # In practice, you would properly tokenize and encode the text queries\n        # For this example, we\'ll use a simplified approach\n\n        # Project and fuse features\n        # This is a simplified version - in practice, you would:\n        # 1. Extract RoI features for detected objects\n        # 2. Encode text queries\n        # 3. Apply cross-attention between visual objects and text\n        # 4. Compute grounding scores\n\n        # Placeholder for actual grounding computation\n        grounding_scores = torch.rand(batch_size, len(detections), 1)  # (batch, num_objects, 1)\n\n        results = []\n        for i in range(batch_size):\n            obj_detections = detections[i]\n            scores = grounding_scores[i].squeeze(-1)\n\n            # Sort by grounding confidence\n            sorted_indices = torch.argsort(scores, descending=True)\n\n            # Apply NMS based on grounding scores\n            selected_indices = torchvision.ops.nms(\n                obj_detections[\'boxes\'][sorted_indices],\n                scores[sorted_indices],\n                iou_threshold=0.5\n            )\n\n            selected_boxes = obj_detections[\'boxes\'][sorted_indices][selected_indices]\n            selected_scores = scores[sorted_indices][selected_indices]\n\n            results.append({\n                \'boxes\': selected_boxes,\n                \'grounding_scores\': selected_scores,\n                \'labels\': obj_detections.get(\'labels\', torch.zeros(len(selected_boxes)))\n            })\n\n        return results\n\nclass AdvancedVisualGrounding(nn.Module):\n    def __init__(self, vision_model_name="openai/clip-vit-base-patch32"):\n        super(AdvancedVisualGrounding, self).__init__()\n\n        # Use CLIP for visual grounding\n        from transformers import CLIPProcessor, CLIPModel\n        self.clip_model = CLIPModel.from_pretrained(vision_model_name)\n        self.clip_processor = CLIPProcessor.from_pretrained(vision_model_name)\n\n        # Additional grounding head\n        self.grounding_head = nn.Linear(512, 1)  # 512 is CLIP\'s feature dimension\n\n    def forward(self, pixel_values, text_descriptions):\n        """\n        Perform visual grounding using CLIP\n\n        Args:\n            pixel_values: Image tensor\n            text_descriptions: List of text descriptions\n        """\n        # Get image and text features from CLIP\n        image_features = self.clip_model.get_image_features(pixel_values=pixel_values)\n\n        # Process text descriptions\n        text_inputs = self.clip_processor.tokenizer(\n            text_descriptions,\n            padding=True,\n            truncation=True,\n            return_tensors="pt"\n        )\n\n        text_features = self.clip_model.get_text_features(**text_inputs)\n\n        # Compute similarity scores\n        # Normalize features\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n\n        # Cosine similarity\n        similarity_scores = torch.matmul(image_features, text_features.T)\n\n        return similarity_scores\n'})}),"\n",(0,a.jsx)(n.h3,{id:"spatial-reasoning-and-layout-understanding",children:"Spatial Reasoning and Layout Understanding"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# spatial_reasoning.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nclass SpatialReasoningModule(nn.Module):\n    def __init__(self, feature_dim=512):\n        super(SpatialReasoningModule, self).__init__()\n\n        self.feature_dim = feature_dim\n\n        # Spatial relationship encoder\n        self.spatial_encoder = nn.Sequential(\n            nn.Linear(4, 128),  # 4 coordinates (x1, y1, x2, y2)\n            nn.ReLU(),\n            nn.Linear(128, feature_dim)\n        )\n\n        # Relative position encoder\n        self.relative_position_encoder = nn.Sequential(\n            nn.Linear(2, 64),  # 2D relative position (dx, dy)\n            nn.ReLU(),\n            nn.Linear(64, feature_dim)\n        )\n\n        # Spatial relation classifier\n        self.relation_classifier = nn.Sequential(\n            nn.Linear(feature_dim * 3, 256),  # obj1 + obj2 + spatial\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 6)  # 6 common spatial relations: left, right, above, below, near, far\n        )\n\n        # Spatial attention mechanism\n        self.spatial_attention = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=8,\n            batch_first=True\n        )\n\n    def compute_spatial_features(self, boxes):\n        """\n        Compute spatial features from bounding boxes\n\n        Args:\n            boxes: Tensor of shape (batch, num_objects, 4) [x1, y1, x2, y2]\n        """\n        # Compute geometric properties\n        x1, y1, x2, y2 = boxes.unbind(-1)\n\n        # Basic spatial features\n        center_x = (x1 + x2) / 2\n        center_y = (y1 + y2) / 2\n        width = x2 - x1\n        height = y2 - y1\n        area = width * height\n\n        # Normalize coordinates (assuming image size normalization)\n        # In practice, you would normalize by image dimensions\n        spatial_features = torch.stack([center_x, center_y, width, height], dim=-1)\n\n        return spatial_features\n\n    def compute_relative_positions(self, boxes):\n        """\n        Compute relative positions between all pairs of objects\n        """\n        batch_size, num_objects, _ = boxes.shape\n\n        # Expand boxes for pairwise comparison\n        boxes_expanded_1 = boxes.unsqueeze(2).expand(-1, -1, num_objects, -1)  # (B, N, N, 4)\n        boxes_expanded_2 = boxes.unsqueeze(1).expand(-1, num_objects, -1, -1)  # (B, N, N, 4)\n\n        # Compute relative centers\n        center1_x = (boxes_expanded_1[..., 0] + boxes_expanded_1[..., 2]) / 2\n        center1_y = (boxes_expanded_1[..., 1] + boxes_expanded_1[..., 3]) / 2\n        center2_x = (boxes_expanded_2[..., 0] + boxes_expanded_2[..., 2]) / 2\n        center2_y = (boxes_expanded_2[..., 1] + boxes_expanded_2[..., 3]) / 2\n\n        # Compute relative positions\n        rel_x = center2_x - center1_x  # (B, N, N)\n        rel_y = center2_y - center1_y  # (B, N, N)\n\n        return torch.stack([rel_x, rel_y], dim=-1)  # (B, N, N, 2)\n\n    def forward(self, visual_features, boxes):\n        """\n        Perform spatial reasoning on visual features and bounding boxes\n\n        Args:\n            visual_features: (batch, num_objects, feature_dim)\n            boxes: (batch, num_objects, 4) [x1, y1, x2, y2]\n        """\n        batch_size, num_objects, _ = visual_features.shape\n\n        # Compute spatial features\n        spatial_features = self.compute_spatial_features(boxes)  # (batch, num_objects, 4)\n        spatial_embeddings = self.spatial_encoder(spatial_features)  # (batch, num_objects, feature_dim)\n\n        # Compute relative positions\n        relative_positions = self.compute_relative_positions(boxes)  # (batch, num_objects, num_objects, 2)\n\n        # Encode relative positions\n        rel_pos_embeddings = self.relative_position_encoder(relative_positions)  # (batch, num_objects, num_objects, feature_dim)\n\n        # Combine visual and spatial features\n        combined_features = visual_features + spatial_embeddings  # (batch, num_objects, feature_dim)\n\n        # Apply spatial attention\n        attended_features, attention_weights = self.spatial_attention(\n            combined_features,  # query\n            combined_features,  # key\n            combined_features   # value\n        )\n\n        # Compute spatial relations between all pairs\n        relation_features = []\n        for i in range(num_objects):\n            for j in range(num_objects):\n                if i != j:\n                    # Combine features of object i and j with spatial relation\n                    obj_pair_features = torch.cat([\n                        attended_features[:, i, :],  # Feature of object i\n                        attended_features[:, j, :],  # Feature of object j\n                        rel_pos_embeddings[:, i, j, :]  # Spatial relation embedding\n                    ], dim=-1)  # (batch, feature_dim * 3)\n\n                    relation_features.append(obj_pair_features)\n\n        # If there are pairs, classify their spatial relations\n        if relation_features:\n            all_relation_features = torch.stack(relation_features, dim=1)  # (batch, num_pairs, feature_dim * 3)\n            spatial_relations = self.relation_classifier(all_relation_features)  # (batch, num_pairs, 6)\n        else:\n            spatial_relations = torch.zeros(batch_size, 0, 6)\n\n        return {\n            \'attended_features\': attended_features,\n            \'spatial_relations\': spatial_relations,\n            \'attention_weights\': attention_weights,\n            \'spatial_features\': spatial_embeddings\n        }\n'})}),"\n",(0,a.jsx)(n.h2,{id:"scene-understanding-with-language",children:"Scene Understanding with Language"}),"\n",(0,a.jsx)(n.h3,{id:"semantic-scene-graph-generation",children:"Semantic Scene Graph Generation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# scene_graph.py\nimport torch\nimport torch.nn as nn\n\nclass SceneGraphGenerator(nn.Module):\n    def __init__(self, vocab_size=10000, feature_dim=512):\n        super(SceneGraphGenerator, self).__init__()\n\n        self.feature_dim = feature_dim\n\n        # Object detection and classification\n        self.object_classifier = nn.Sequential(\n            nn.Linear(feature_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, vocab_size)  # Number of object categories\n        )\n\n        # Relationship classifier\n        self.relation_classifier = nn.Sequential(\n            nn.Linear(feature_dim * 2 + 4, 512),  # 2 obj features + 4 spatial coords\n            nn.ReLU(),\n            nn.Linear(512, 100)  # Number of relationship types\n        )\n\n        # Spatial feature encoder\n        self.spatial_encoder = nn.Linear(4, 128)  # [x1, y1, x2, y2]\n\n    def forward(self, visual_features, boxes):\n        \"\"\"\n        Generate scene graph from visual features and bounding boxes\n\n        Args:\n            visual_features: (batch, num_objects, feature_dim)\n            boxes: (batch, num_objects, 4) [x1, y1, x2, y2]\n        \"\"\"\n        batch_size, num_objects, _ = visual_features.shape\n\n        # Classify objects\n        object_logits = self.object_classifier(visual_features)  # (batch, num_objects, vocab_size)\n        object_classes = torch.argmax(object_logits, dim=-1)  # (batch, num_objects)\n\n        # Encode spatial features\n        spatial_features = self.spatial_encoder(boxes)  # (batch, num_objects, 128)\n\n        # Compute relationships between all pairs\n        relationships = []\n        for i in range(num_objects):\n            for j in range(num_objects):\n                if i != j:\n                    # Combine features of object i and j with spatial features\n                    spatial_rel = torch.cat([\n                        boxes[:, i, :],  # Box of subject\n                        boxes[:, j, :]   # Box of object\n                    ], dim=-1)  # (batch, 8)\n\n                    spatial_rel_enc = self.spatial_encoder(spatial_rel)  # (batch, 128)\n\n                    rel_features = torch.cat([\n                        visual_features[:, i, :],  # Subject features\n                        visual_features[:, j, :],  # Object features\n                        spatial_rel_enc            # Spatial relationship\n                    ], dim=-1)  # (batch, feature_dim * 2 + 128)\n\n                    rel_logits = self.relation_classifier(rel_features)  # (batch, num_relations)\n                    rel_class = torch.argmax(rel_logits, dim=-1)  # (batch,)\n\n                    # Store relationship: (subject_idx, object_idx, relation_class)\n                    relationships.append(torch.stack([\n                        torch.full((batch_size,), i),\n                        torch.full((batch_size,), j),\n                        rel_class\n                    ], dim=-1))\n\n        if relationships:\n            all_relationships = torch.stack(relationships, dim=1)  # (batch, num_pairs, 3)\n        else:\n            all_relationships = torch.zeros(batch_size, 0, 3, dtype=torch.long)\n\n        return {\n            'object_classes': object_classes,\n            'relationships': all_relationships,\n            'object_logits': object_logits\n        }\n"})}),"\n",(0,a.jsx)(n.h3,{id:"language-grounded-scene-understanding",children:"Language-Grounded Scene Understanding"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# grounded_scene_understanding.py\nimport torch\nimport torch.nn as nn\n\nclass GroundedSceneUnderstanding(nn.Module):\n    def __init__(self, vision_model_name="openai/clip-vit-base-patch32",\n                 language_model_name="meta-llama/Llama-2-7b-hf"):\n        super(GroundedSceneUnderstanding, self).__init__()\n\n        # Visual and language encoders\n        from transformers import CLIPModel, LlamaModel\n        self.clip_model = CLIPModel.from_pretrained(vision_model_name)\n        self.llm = LlamaModel.from_pretrained(language_model_name)\n\n        # Fusion module\n        self.fusion = nn.MultiheadAttention(\n            embed_dim=512,  # CLIP\'s feature dimension\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Scene understanding head\n        self.scene_classifier = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 100)  # 100 scene categories\n        )\n\n        # Object detection head (simplified)\n        self.object_detector = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 4 + 1)  # 4 for bbox + 1 for confidence\n        )\n\n    def forward(self, pixel_values, text_queries):\n        """\n        Understand scene based on visual input and language queries\n\n        Args:\n            pixel_values: Image tensor\n            text_queries: Text queries about the scene\n        """\n        # Extract visual features\n        visual_features = self.clip_model.get_image_features(pixel_values=pixel_values)\n\n        # Process text queries\n        # This is simplified - in practice, you would tokenize and encode the text\n        # For now, we\'ll use the last hidden state of the language model as text features\n        batch_size = pixel_values.size(0)\n\n        # In a real implementation, you would process text_queries through the LLM\n        # For this example, we\'ll create dummy text features\n        text_features = torch.randn(batch_size, 1, 512, device=pixel_values.device)\n\n        # Apply cross-modal attention\n        attended_features, attention_weights = self.fusion(\n            visual_features.unsqueeze(1),  # query: (batch, 1, 512)\n            text_features,                 # key: (batch, seq_len, 512)\n            text_features                  # value: (batch, seq_len, 512)\n        )\n\n        # Scene classification\n        scene_logits = self.scene_classifier(attended_features.squeeze(1))  # (batch, 100)\n\n        # Object detection (simplified)\n        object_predictions = self.object_detector(visual_features)  # (batch, 4+1)\n\n        return {\n            \'scene_classification\': scene_logits,\n            \'object_predictions\': object_predictions,\n            \'attention_weights\': attention_weights,\n            \'fused_features\': attended_features\n        }\n\nclass MultimodalSceneGraph(nn.Module):\n    def __init__(self, vocab_size=10000, feature_dim=512):\n        super(MultimodalSceneGraph, self).__init__()\n\n        self.feature_dim = feature_dim\n\n        # Visual feature encoder\n        self.visual_encoder = nn.Sequential(\n            nn.Linear(768, feature_dim),  # CLIP visual features\n            nn.LayerNorm(feature_dim),\n            nn.ReLU()\n        )\n\n        # Language feature encoder\n        self.language_encoder = nn.Sequential(\n            nn.Linear(4096, feature_dim),  # LLM hidden size\n            nn.LayerNorm(feature_dim),\n            nn.ReLU()\n        )\n\n        # Multimodal fusion\n        self.fusion = nn.MultiheadAttention(\n            embed_dim=feature_dim,\n            num_heads=8,\n            dropout=0.1,\n            batch_first=True\n        )\n\n        # Scene graph generation\n        self.graph_generator = SceneGraphGenerator(vocab_size, feature_dim)\n\n    def forward(self, visual_features, language_features, boxes):\n        """\n        Generate multimodal scene graph\n\n        Args:\n            visual_features: Visual features from CLIP\n            language_features: Language features from LLM\n            boxes: Bounding boxes for objects\n        """\n        batch_size = visual_features.size(0)\n\n        # Encode features\n        vis_encoded = self.visual_encoder(visual_features)  # (batch, num_objects, feature_dim)\n        lang_encoded = self.language_encoder(language_features)  # (batch, seq_len, feature_dim)\n\n        # Apply multimodal fusion\n        fused_features, attention_weights = self.fusion(\n            vis_encoded,  # query\n            lang_encoded,  # key\n            lang_encoded   # value\n        )\n\n        # Generate scene graph using fused features\n        scene_graph = self.graph_generator(fused_features, boxes)\n\n        return {\n            **scene_graph,\n            \'attention_weights\': attention_weights,\n            \'fused_features\': fused_features\n        }\n'})}),"\n",(0,a.jsx)(n.h2,{id:"implementation-and-integration",children:"Implementation and Integration"}),"\n",(0,a.jsx)(n.h3,{id:"complete-multimodal-perception-system",children:"Complete Multimodal Perception System"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# complete_multimodal_system.py\nimport torch\nimport torch.nn as nn\n\nclass CompleteMultimodalPerception(nn.Module):\n    def __init__(self, language_model_name=\"meta-llama/Llama-2-7b-hf\"):\n        super(CompleteMultimodalPerception, self).__init__()\n\n        # Visual processing components\n        self.visual_processor = VisualProcessor()\n\n        # Language processing components\n        self.language_processor = LanguageProcessor(language_model_name)\n\n        # Multimodal fusion\n        self.fusion = MultimodalFusion()\n\n        # Spatial reasoning\n        self.spatial_reasoner = SpatialReasoningModule()\n\n        # Scene understanding\n        self.scene_graph = MultimodalSceneGraph()\n\n        # Grounding components\n        self.grounding_system = AdvancedVisualGrounding()\n\n    def forward(self, images, text_queries, boxes=None):\n        \"\"\"\n        Complete multimodal perception pipeline\n\n        Args:\n            images: Batch of images\n            text_queries: Text queries for grounding\n            boxes: Optional bounding boxes for objects\n        \"\"\"\n        # Visual processing\n        visual_outputs = self.visual_processor(images)\n\n        # Language processing\n        tokenized = self.language_processor.tokenize_text(text_queries)\n        language_outputs = self.language_processor(\n            tokenized['input_ids'],\n            tokenized['attention_mask'],\n            visual_outputs['features']\n        )\n\n        # Multimodal fusion\n        fusion_outputs = self.fusion(\n            visual_outputs['features'],\n            language_outputs['features']\n        )\n\n        # Spatial reasoning (if boxes provided)\n        spatial_outputs = None\n        if boxes is not None:\n            spatial_outputs = self.spatial_reasoner(\n                visual_outputs['spatial_features'],\n                boxes\n            )\n\n        # Scene graph generation\n        scene_outputs = self.scene_graph(\n            visual_outputs['features'][:, 1:, :],  # Exclude global feature\n            language_outputs['features'],\n            boxes if boxes is not None else torch.zeros(1, 1, 4)  # Placeholder\n        )\n\n        # Visual grounding\n        grounding_outputs = self.grounding_system(images, text_queries)\n\n        return {\n            'visual_features': visual_outputs,\n            'language_features': language_outputs,\n            'fused_features': fusion_outputs,\n            'spatial_reasoning': spatial_outputs,\n            'scene_graph': scene_outputs,\n            'grounding': grounding_outputs\n        }\n\n    def predict_object_grounding(self, images, text_query):\n        \"\"\"\n        Convenience method for object grounding\n        \"\"\"\n        outputs = self(images, [text_query])\n\n        # Extract grounding results\n        grounding_scores = outputs['grounding']\n\n        # Return the object in the image that best matches the text query\n        best_match_idx = torch.argmax(grounding_scores[0])\n\n        return {\n            'best_match_score': grounding_scores[0][best_match_idx].item(),\n            'all_scores': grounding_scores[0].tolist()\n        }\n\n# Example usage and training\nclass MultimodalPerceptionTrainer:\n    def __init__(self, model, learning_rate=1e-4):\n        self.model = model\n        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n        self.criterion = nn.CrossEntropyLoss()\n\n    def train_step(self, images, text_queries, ground_truth_labels):\n        \"\"\"\n        Single training step\n        \"\"\"\n        self.model.train()\n\n        # Forward pass\n        outputs = self.model(images, text_queries)\n\n        # Compute loss (this is simplified - in practice, you'd have specific losses\n        # for each component: grounding loss, scene understanding loss, etc.)\n        loss = torch.tensor(0.0, requires_grad=True)  # Placeholder\n\n        # For demonstration, let's compute a simple grounding loss\n        if 'grounding' in outputs:\n            # This would compare with ground truth object associations\n            pass\n\n        # Backward pass\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n        self.optimizer.step()\n\n        return loss.item()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"evaluation-and-benchmarking",children:"Evaluation and Benchmarking"}),"\n",(0,a.jsx)(n.h3,{id:"multimodal-perception-metrics",children:"Multimodal Perception Metrics"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# evaluation_metrics.py\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nclass MultimodalEvaluation:\n    def __init__(self):\n        self.metrics = {}\n\n    def evaluate_grounding_accuracy(self, predicted_boxes, ground_truth_boxes, iou_threshold=0.5):\n        """Evaluate visual grounding accuracy"""\n        ious = []\n        correct_groundings = 0\n        total_groundings = len(predicted_boxes)\n\n        for pred_box, gt_box in zip(predicted_boxes, ground_truth_boxes):\n            # Calculate IoU\n            iou = self.calculate_iou(pred_box, gt_box)\n            ious.append(iou)\n\n            if iou >= iou_threshold:\n                correct_groundings += 1\n\n        accuracy = correct_groundings / total_groundings if total_groundings > 0 else 0\n        mean_iou = np.mean(ious) if ious else 0\n\n        return {\n            \'grounding_accuracy\': accuracy,\n            \'mean_iou\': mean_iou,\n            \'ious\': ious\n        }\n\n    def calculate_iou(self, box1, box2):\n        """Calculate Intersection over Union"""\n        # box format: [x1, y1, x2, y2]\n        x1_inter = max(box1[0], box2[0])\n        y1_inter = max(box1[1], box2[1])\n        x2_inter = min(box1[2], box2[2])\n        y2_inter = min(box1[3], box2[3])\n\n        if x2_inter <= x1_inter or y2_inter <= y1_inter:\n            return 0.0\n\n        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n        union_area = area1 + area2 - inter_area\n\n        return inter_area / union_area if union_area > 0 else 0.0\n\n    def evaluate_language_understanding(self, predicted_actions, ground_truth_actions, language_commands):\n        """Evaluate how well the system understands language in context of perception"""\n        # Calculate action accuracy for each language command\n        action_accuracy = accuracy_score(\n            ground_truth_actions.argmax(axis=1),\n            predicted_actions.argmax(axis=1)\n        )\n\n        # Calculate command-specific accuracy\n        command_accuracy = {}\n        for cmd in set(language_commands):\n            cmd_mask = np.array(language_commands) == cmd\n            if np.sum(cmd_mask) > 0:\n                cmd_acc = accuracy_score(\n                    np.array(ground_truth_actions)[cmd_mask].argmax(axis=1),\n                    np.array(predicted_actions)[cmd_mask].argmax(axis=1)\n                )\n                command_accuracy[cmd] = cmd_acc\n\n        return {\n            \'overall_accuracy\': action_accuracy,\n            \'command_accuracy\': command_accuracy\n        }\n\n    def generate_comprehensive_report(self, model, test_dataset):\n        """Generate comprehensive evaluation report"""\n        all_grounding_results = []\n        all_language_results = []\n\n        for batch in test_dataset:\n            # Run inference\n            with torch.no_grad():\n                outputs = model(\n                    batch[\'images\'],\n                    batch[\'text_queries\'],\n                    batch.get(\'boxes\')\n                )\n\n            # Evaluate grounding if ground truth is available\n            if \'ground_truth_boxes\' in batch:\n                grounding_eval = self.evaluate_grounding_accuracy(\n                    outputs[\'grounding_predictions\'],\n                    batch[\'ground_truth_boxes\']\n                )\n                all_grounding_results.append(grounding_eval)\n\n            # Evaluate language understanding\n            if \'ground_truth_actions\' in batch:\n                language_eval = self.evaluate_language_understanding(\n                    outputs[\'predicted_actions\'],\n                    batch[\'ground_truth_actions\'],\n                    batch[\'language_commands\']\n                )\n                all_language_results.append(language_eval)\n\n        # Aggregate results\n        avg_grounding_acc = np.mean([r[\'grounding_accuracy\'] for r in all_grounding_results])\n        avg_mean_iou = np.mean([r[\'mean_iou\'] for r in all_grounding_results])\n\n        report = f"""\n        Multimodal Perception Evaluation Report\n        ======================================\n\n        Visual Grounding Performance:\n        - Grounding Accuracy: {avg_grounding_acc:.3f}\n        - Mean IoU: {avg_mean_iou:.3f}\n\n        Language Understanding:\n        - Overall Action Accuracy: {(sum(r[\'overall_accuracy\'] for r in all_language_results) / len(all_language_results)):.3f}\n\n        System Capabilities:\n        """\n\n        if avg_grounding_acc > 0.7:\n            report += "- Strong visual grounding capabilities\\n"\n        elif avg_grounding_acc > 0.5:\n            report += "- Moderate visual grounding capabilities\\n"\n        else:\n            report += "- Visual grounding needs improvement\\n"\n\n        if avg_mean_iou > 0.5:\n            report += "- Good spatial precision\\n"\n        else:\n            report += "- Spatial precision needs improvement\\n"\n\n        return report\n'})}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Multimodal perception forms the foundation of Vision-Language-Action systems, enabling robots to understand their environment through both visual and linguistic modalities. The integration of visual processing, language understanding, and spatial reasoning creates a comprehensive system that can ground language commands to specific visual objects and locations."}),"\n",(0,a.jsx)(n.p,{children:"Key components include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Visual processing pipelines that extract meaningful features from images"}),"\n",(0,a.jsx)(n.li,{children:"Language processing systems that understand natural language commands"}),"\n",(0,a.jsx)(n.li,{children:"Multimodal fusion mechanisms that combine information from different modalities"}),"\n",(0,a.jsx)(n.li,{children:"Spatial reasoning modules that understand object relationships and positions"}),"\n",(0,a.jsx)(n.li,{children:"Grounding systems that connect language to visual elements"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"These components work together to enable robots to perceive and understand their environment in a human-like manner, setting the stage for language-guided action and manipulation systems in the next section."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}}}]);