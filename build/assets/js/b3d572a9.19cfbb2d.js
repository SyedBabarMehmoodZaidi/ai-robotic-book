"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[174],{7289:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var a=t(4848),i=t(8453);const o={sidebar_position:4},s="Language-Guided Manipulation: From Commands to Actions",r={id:"module-4-vla/language-guided-manipulation",title:"Language-Guided Manipulation: From Commands to Actions",description:"Learning Objectives",source:"@site/docs/module-4-vla/language-guided-manipulation.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/language-guided-manipulation",permalink:"/ai-robotic-book/docs/module-4-vla/language-guided-manipulation",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/ai-robotic-book/tree/main/docs/module-4-vla/language-guided-manipulation.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Multimodal Perception: Vision-Language Integration",permalink:"/ai-robotic-book/docs/module-4-vla/multimodal-perception"},next:{title:"VLA System Integration: End-to-End Implementation",permalink:"/ai-robotic-book/docs/module-4-vla/vla-system-integration"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Language-Guided Manipulation",id:"introduction-to-language-guided-manipulation",level:2},{value:"Language Command Processing",id:"language-command-processing",level:2},{value:"Command Parsing and Understanding",id:"command-parsing-and-understanding",level:3},{value:"Command-to-Action Mapping",id:"command-to-action-mapping",level:3},{value:"Action Planning and Execution",id:"action-planning-and-execution",level:2},{value:"Skill-Based Manipulation Framework",id:"skill-based-manipulation-framework",level:3},{value:"Motion Planning Integration",id:"motion-planning-integration",level:3},{value:"Perception-Action Integration",id:"perception-action-integration",level:2},{value:"Object Grounding and Manipulation",id:"object-grounding-and-manipulation",level:3},{value:"Implementation Examples",id:"implementation-examples",level:2},{value:"Complete Language-Guided Manipulation System",id:"complete-language-guided-manipulation-system",level:3},{value:"Real-World Integration Example",id:"real-world-integration-example",level:3},{value:"Evaluation and Performance Metrics",id:"evaluation-and-performance-metrics",level:2},{value:"Manipulation Performance Evaluation",id:"manipulation-performance-evaluation",level:3},{value:"Summary",id:"summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.h1,{id:"language-guided-manipulation-from-commands-to-actions",children:"Language-Guided Manipulation: From Commands to Actions"}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand the principles of language-guided robotic manipulation"}),"\n",(0,a.jsx)(e.li,{children:"Implement natural language processing for robotic command interpretation"}),"\n",(0,a.jsx)(e.li,{children:"Create action planning systems that execute language commands"}),"\n",(0,a.jsx)(e.li,{children:"Design skill-based manipulation frameworks with language grounding"}),"\n",(0,a.jsx)(e.li,{children:"Integrate perception systems with language-guided control"}),"\n",(0,a.jsx)(e.li,{children:"Evaluate language-guided manipulation performance and robustness"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-language-guided-manipulation",children:"Introduction to Language-Guided Manipulation"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Language-guided manipulation"})," enables robots to understand and execute natural language commands for object manipulation tasks. This capability allows humans to interact with robots using intuitive, high-level instructions rather than low-level programming or teleoperation."]}),"\n",(0,a.jsx)(e.p,{children:"Language-guided manipulation involves:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Natural Language Understanding"}),": Interpreting human commands"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action Planning"}),": Converting language to executable actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Perception Integration"}),": Connecting language to visual objects"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Skill Execution"}),": Performing manipulation tasks based on commands"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Feedback and Correction"}),": Handling ambiguous or incorrect commands"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"language-command-processing",children:"Language Command Processing"}),"\n",(0,a.jsx)(e.h3,{id:"command-parsing-and-understanding",children:"Command Parsing and Understanding"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# command_parser.py\nimport torch\nimport torch.nn as nn\nimport re\nfrom transformers import LlamaModel, LlamaTokenizer\nfrom typing import Dict, List, Tuple, Optional\n\nclass CommandParser(nn.Module):\n    def __init__(self, model_name=\"meta-llama/Llama-2-7b-hf\"):\n        super(CommandParser, self).__init__()\n\n        self.tokenizer = LlamaTokenizer.from_pretrained(model_name)\n        self.llm = LlamaModel.from_pretrained(model_name)\n\n        # Add special tokens for manipulation commands\n        special_tokens = {\n            'additional_special_tokens': [\n                '<ACTION>', '</ACTION>',\n                '<OBJECT>', '</OBJECT>',\n                '<LOCATION>', '</LOCATION>',\n                '<GRASP>', '</GRASP>',\n                '<PLACE>', '</PLACE>',\n                '<MOVE>', '</MOVE>'\n            ]\n        }\n        self.tokenizer.add_special_tokens(special_tokens)\n\n        # Command classification head\n        self.command_classifier = nn.Sequential(\n            nn.Linear(self.llm.config.hidden_size, 512),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, 10)  # 10 common manipulation actions\n        )\n\n        # Argument extraction head\n        self.argument_extractor = nn.Sequential(\n            nn.Linear(self.llm.config.hidden_size, 512),\n            nn.ReLU(),\n            nn.Linear(512, 3)  # object, location, grasp type\n        )\n\n        # Action sequence generator\n        self.action_generator = nn.Sequential(\n            nn.Linear(self.llm.config.hidden_size, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 50)  # Maximum 50 actions in sequence\n        )\n\n    def parse_command(self, command: str) -> Dict:\n        \"\"\"\n        Parse a natural language command into structured components\n        \"\"\"\n        # Tokenize the command\n        inputs = self.tokenizer(\n            command,\n            return_tensors='pt',\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n\n        # Get language features\n        outputs = self.llm(**inputs)\n        hidden_states = outputs.last_hidden_state  # (1, seq_len, hidden_size)\n\n        # Get command classification\n        command_logits = self.command_classifier(hidden_states[:, -1, :])  # Use last token\n        command_type = torch.argmax(command_logits, dim=-1).item()\n\n        # Extract arguments\n        arguments = self.argument_extractor(hidden_states[:, -1, :])\n\n        # Generate action sequence\n        action_seq = self.action_generator(hidden_states[:, -1, :])\n\n        return {\n            'command_type': command_type,\n            'arguments': arguments,\n            'action_sequence': action_seq,\n            'raw_command': command\n        }\n\n    def extract_entities(self, command: str) -> Dict[str, List[str]]:\n        \"\"\"\n        Extract objects, locations, and other entities from command\n        \"\"\"\n        # Simple pattern matching for demonstration\n        # In practice, use more sophisticated NLP techniques\n        entities = {\n            'objects': [],\n            'locations': [],\n            'actions': [],\n            'modifiers': []\n        }\n\n        # Common objects\n        object_patterns = [\n            r'\\b(block|cube|ball|cup|bottle|box|container|toy|book|pen|phone|keys|apple|banana)\\b',\n            r'\\b(red|blue|green|yellow|large|small|big|tiny|heavy|light)\\b'\n        ]\n\n        # Common locations\n        location_patterns = [\n            r'\\b(table|shelf|counter|floor|box|bin|cabinet|drawer|fridge|microwave)\\b',\n            r'\\b(left|right|center|middle|front|back|near|beside|on|in|under|above)\\b'\n        ]\n\n        # Common actions\n        action_patterns = [\n            r'\\b(pick|grasp|take|lift|hold|move|place|put|set|drop|release|transfer)\\b'\n        ]\n\n        for pattern in object_patterns[0].split('|'):\n            pattern = pattern.strip('()')\n            matches = re.findall(pattern, command, re.IGNORECASE)\n            entities['objects'].extend(matches)\n\n        for pattern in location_patterns[0].split('|'):\n            pattern = pattern.strip('()')\n            matches = re.findall(pattern, command, re.IGNORECASE)\n            entities['locations'].extend(matches)\n\n        for pattern in action_patterns[0].split('|'):\n            pattern = pattern.strip('()')\n            matches = re.findall(pattern, command, re.IGNORECASE)\n            entities['actions'].extend(matches)\n\n        return entities\n\nclass AdvancedCommandParser(nn.Module):\n    def __init__(self, vocab_size=32000, hidden_dim=4096):\n        super(AdvancedCommandParser, self).__init__()\n\n        self.hidden_dim = hidden_dim\n\n        # Semantic role labeling for command understanding\n        self.semantic_analyzer = nn.Sequential(\n            nn.Linear(hidden_dim, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 20)  # 20 semantic roles (agent, theme, goal, etc.)\n        )\n\n        # Task decomposition network\n        self.task_decomposer = nn.Sequential(\n            nn.Linear(hidden_dim, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 100)  # Maximum 100 subtasks\n        )\n\n        # Grasp type classifier\n        self.grasp_classifier = nn.Sequential(\n            nn.Linear(hidden_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 5)  # 5 grasp types: power, precision, pinch, etc.\n        )\n\n        # Motion primitive selector\n        self.motion_selector = nn.Sequential(\n            nn.Linear(hidden_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 20)  # 20 common motion primitives\n        )\n\n    def forward(self, command_features, visual_context=None):\n        \"\"\"\n        Analyze command and generate structured representation\n\n        Args:\n            command_features: Language features from LLM\n            visual_context: Optional visual context features\n        \"\"\"\n        batch_size = command_features.size(0)\n\n        # Semantic role analysis\n        semantic_roles = self.semantic_analyzer(command_features)  # (batch, 20)\n\n        # Task decomposition\n        task_decomposition = self.task_decomposer(command_features)  # (batch, 100)\n        # Apply softmax and get top tasks\n        task_probs = torch.softmax(task_decomposition, dim=-1)\n        top_tasks = torch.topk(task_probs, k=5, dim=-1)  # Top 5 subtasks\n\n        # Grasp type prediction\n        grasp_types = self.grasp_classifier(command_features)  # (batch, 5)\n\n        # Motion primitive selection\n        motion_primitives = self.motion_selector(command_features)  # (batch, 20)\n\n        return {\n            'semantic_roles': semantic_roles,\n            'task_decomposition': {\n                'tasks': top_tasks.indices,\n                'probabilities': top_tasks.values\n            },\n            'grasp_types': grasp_types,\n            'motion_primitives': motion_primitives\n        }\n"})}),"\n",(0,a.jsx)(e.h3,{id:"command-to-action-mapping",children:"Command-to-Action Mapping"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# command_action_mapping.py\nimport torch\nimport torch.nn as nn\nfrom enum import Enum\n\nclass ManipulationAction(Enum):\n    PICK = 0\n    PLACE = 1\n    MOVE = 2\n    GRASP = 3\n    RELEASE = 4\n    APPROACH = 5\n    RETRACT = 6\n    ALIGN = 7\n    INSERT = 8\n    EXTRACT = 9\n\nclass CommandActionMapper(nn.Module):\n    def __init__(self, hidden_dim=4096, action_dim=7):\n        super(CommandActionMapper, self).__init__()\n\n        self.action_dim = action_dim\n\n        # Command embedding to action mapping\n        self.command_to_action = nn.Sequential(\n            nn.Linear(hidden_dim, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, action_dim * 10)  # 10 time steps\n        )\n\n        # Action type classifier\n        self.action_type_classifier = nn.Linear(hidden_dim, len(ManipulationAction))\n\n        # Object-specific action modifier\n        self.object_action_modifier = nn.Sequential(\n            nn.Linear(hidden_dim + 512, 512),  # command + object features\n            nn.ReLU(),\n            nn.Linear(512, action_dim)\n        )\n\n    def forward(self, command_features, object_features=None):\n        \"\"\"\n        Map command features to robot actions\n\n        Args:\n            command_features: Language features from command\n            object_features: Optional object-specific features\n        \"\"\"\n        batch_size = command_features.size(0)\n\n        # Map command to sequence of actions\n        action_sequence = self.command_to_action(command_features)\n        action_sequence = action_sequence.view(batch_size, 10, self.action_dim)  # (batch, 10, action_dim)\n\n        # Classify action type\n        action_types = self.action_type_classifier(command_features)  # (batch, num_action_types)\n\n        # If object features provided, modify actions accordingly\n        if object_features is not None:\n            combined_features = torch.cat([command_features, object_features], dim=-1)\n            action_modifiers = self.object_action_modifier(combined_features)  # (batch, action_dim)\n\n            # Apply modifiers to the action sequence\n            modified_actions = action_sequence + action_modifiers.unsqueeze(1)  # Broadcast to sequence\n        else:\n            modified_actions = action_sequence\n\n        return {\n            'action_sequence': modified_actions,\n            'action_types': action_types,\n            'predicted_actions': modified_actions[:, 0, :]  # First action in sequence\n        }\n\nclass HierarchicalCommandProcessor(nn.Module):\n    def __init__(self, hidden_dim=4096):\n        super(HierarchicalCommandProcessor, self).__init__()\n\n        # High-level task planner\n        self.task_planner = nn.Sequential(\n            nn.Linear(hidden_dim, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 50)  # 50 possible high-level tasks\n        )\n\n        # Skill selector\n        self.skill_selector = nn.Sequential(\n            nn.Linear(hidden_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, 20)  # 20 manipulation skills\n        )\n\n        # Skill parameter generator\n        self.skill_parameter_generator = nn.Sequential(\n            nn.Linear(hidden_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, 100)  # 100 possible skill parameters\n        )\n\n        # Skill execution sequence\n        self.skill_execution_planner = nn.Sequential(\n            nn.Linear(hidden_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, 50),  # Sequence of 50 skill steps\n            nn.Softmax(dim=-1)\n        )\n\n    def forward(self, command_features):\n        \"\"\"\n        Process command hierarchically: task -> skills -> parameters -> execution\n        \"\"\"\n        batch_size = command_features.size(0)\n\n        # Plan high-level task\n        task_plan = self.task_planner(command_features)  # (batch, 50)\n        selected_task = torch.argmax(task_plan, dim=-1)  # (batch,)\n\n        # Select appropriate skills\n        skill_scores = self.skill_selector(command_features)  # (batch, 20)\n        selected_skills = torch.topk(skill_scores, k=3, dim=-1)  # Top 3 skills\n\n        # Generate skill parameters\n        skill_params = self.skill_parameter_generator(command_features)  # (batch, 100)\n\n        # Plan skill execution sequence\n        execution_sequence = self.skill_execution_planner(command_features)  # (batch, 50)\n\n        return {\n            'selected_task': selected_task,\n            'selected_skills': selected_skills.indices,\n            'skill_parameters': skill_params,\n            'execution_sequence': execution_sequence\n        }\n"})}),"\n",(0,a.jsx)(e.h2,{id:"action-planning-and-execution",children:"Action Planning and Execution"}),"\n",(0,a.jsx)(e.h3,{id:"skill-based-manipulation-framework",children:"Skill-Based Manipulation Framework"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# skill_framework.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Dict, List, Tuple\n\nclass ManipulationSkill:\n    def __init__(self, name: str, skill_id: int):\n        self.name = name\n        self.skill_id = skill_id\n        self.primitive_actions = []\n        self.preconditions = []\n        self.postconditions = []\n\n    def execute(self, robot_state, target_object, target_location):\n        """Execute the skill with given parameters"""\n        raise NotImplementedError\n\nclass PickSkill(ManipulationSkill):\n    def __init__(self):\n        super().__init__("pick", 0)\n        self.preconditions = ["object_visible", "gripper_open", "not_holding_object"]\n        self.postconditions = ["object_grasped", "gripper_closed"]\n\n    def execute(self, robot_state, target_object, target_location=None):\n        """Execute pick action"""\n        # Implementation would interface with robot controller\n        actions = [\n            {"type": "approach", "object": target_object, "offset": 0.1},\n            {"type": "grasp", "object": target_object},\n            {"type": "lift", "height": 0.1}\n        ]\n        return actions\n\nclass PlaceSkill(ManipulationSkill):\n    def __init__(self):\n        super().__init__("place", 1)\n        self.preconditions = ["holding_object", "gripper_closed"]\n        self.postconditions = ["object_released", "gripper_open", "object_placed"]\n\n    def execute(self, robot_state, target_object, target_location):\n        """Execute place action"""\n        actions = [\n            {"type": "approach", "location": target_location, "offset": 0.1},\n            {"type": "align", "location": target_location},\n            {"type": "release", "location": target_location},\n            {"type": "retract", "distance": 0.1}\n        ]\n        return actions\n\nclass SkillLibrary(nn.Module):\n    def __init__(self, skill_dim=256):\n        super(SkillLibrary, self).__init__()\n\n        self.skills = {\n            0: PickSkill(),\n            1: PlaceSkill(),\n            # Add more skills as needed\n        }\n\n        # Skill embedding network\n        self.skill_embedder = nn.Sequential(\n            nn.Linear(4096, 512),  # Language features -> skill features\n            nn.ReLU(),\n            nn.Linear(512, skill_dim),\n            nn.LayerNorm(skill_dim)\n        )\n\n        # Skill selector\n        self.skill_selector = nn.Sequential(\n            nn.Linear(skill_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, len(self.skills))\n        )\n\n        # Skill parameter generator\n        self.skill_parameter_generator = nn.Sequential(\n            nn.Linear(skill_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 50)  # 50 possible parameters\n        )\n\n    def select_skill(self, command_features):\n        """Select appropriate skill based on command"""\n        skill_features = self.skill_embedder(command_features)\n        skill_logits = self.skill_selector(skill_features)\n        skill_probs = torch.softmax(skill_logits, dim=-1)\n\n        # Get most likely skill\n        selected_skill_id = torch.argmax(skill_probs, dim=-1)\n\n        return {\n            \'skill_id\': selected_skill_id.item(),\n            \'skill_probabilities\': skill_probs,\n            \'skill_features\': skill_features\n        }\n\n    def generate_skill_parameters(self, command_features, skill_features):\n        """Generate parameters for selected skill"""\n        combined_features = torch.cat([command_features, skill_features], dim=-1)\n        parameters = self.skill_parameter_generator(combined_features)\n\n        return parameters\n\nclass SkillExecutor(nn.Module):\n    def __init__(self):\n        super(SkillExecutor, self).__init__()\n\n        self.skill_library = SkillLibrary()\n\n    def forward(self, command_features, visual_objects, target_object_id, target_location=None):\n        """\n        Execute manipulation skill based on command and visual context\n\n        Args:\n            command_features: Language features from command\n            visual_objects: Detected objects with features\n            target_object_id: ID of target object to manipulate\n            target_location: Optional target location for placement\n        """\n        # Select skill based on command\n        skill_selection = self.skill_library.select_skill(command_features)\n        skill_id = skill_selection[\'skill_id\']\n\n        # Get target object features\n        if target_object_id < len(visual_objects):\n            target_features = visual_objects[target_object_id][\'features\']\n        else:\n            target_features = torch.zeros(512)  # Default features\n\n        # Generate skill parameters\n        skill_params = self.skill_library.generate_skill_parameters(\n            command_features,\n            skill_selection[\'skill_features\']\n        )\n\n        # Execute the selected skill\n        selected_skill = self.skill_library.skills[skill_id]\n\n        # In a real implementation, this would interface with robot controller\n        # For this example, we\'ll return a structured action plan\n        action_plan = {\n            \'skill_name\': selected_skill.name,\n            \'skill_id\': skill_id,\n            \'target_object\': target_object_id,\n            \'target_location\': target_location,\n            \'parameters\': skill_params,\n            \'preconditions\': selected_skill.preconditions,\n            \'postconditions\': selected_skill.postconditions\n        }\n\n        return action_plan\n'})}),"\n",(0,a.jsx)(e.h3,{id:"motion-planning-integration",children:"Motion Planning Integration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# motion_planning_integration.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nclass MotionPlanner(nn.Module):\n    def __init__(self, action_dim=7):\n        super(MotionPlanner, self).__init__()\n\n        self.action_dim = action_dim\n\n        # Trajectory generator\n        self.trajectory_generator = nn.Sequential(\n            nn.Linear(512, 512),  # Skill + object features\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 100 * action_dim),  # 100 waypoints\n            nn.Tanh()\n        )\n\n        # Collision avoidance network\n        self.collision_avoider = nn.Sequential(\n            nn.Linear(512 + 64, 256),  # skill + environment features\n            nn.ReLU(),\n            nn.Linear(256, action_dim),\n            nn.Tanh()\n        )\n\n        # Gripper controller\n        self.gripper_controller = nn.Sequential(\n            nn.Linear(512, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),  # gripper width\n            nn.Sigmoid()\n        )\n\n    def plan_trajectory(self, skill_features, object_features, environment_features):\n        \"\"\"\n        Plan motion trajectory based on skill and environment\n\n        Args:\n            skill_features: Features of selected manipulation skill\n            object_features: Features of target object\n            environment_features: Features of environment/obstacles\n        \"\"\"\n        # Combine features for trajectory planning\n        combined_features = torch.cat([skill_features, object_features], dim=-1)\n\n        # Generate trajectory\n        trajectory_flat = self.trajectory_generator(combined_features)\n        trajectory = trajectory_flat.view(-1, 100, self.action_dim)  # (batch, 100, action_dim)\n\n        # Apply collision avoidance\n        env_augmented = torch.cat([skill_features, environment_features], dim=-1)\n        collision_adjustments = self.collision_avoider(env_augmented)\n\n        # Adjust trajectory with collision avoidance\n        adjusted_trajectory = trajectory + collision_adjustments.unsqueeze(1)\n\n        # Generate gripper commands\n        gripper_commands = self.gripper_controller(skill_features)  # (batch, 1)\n\n        return {\n            'trajectory': adjusted_trajectory,\n            'gripper_commands': gripper_commands,\n            'collision_adjustments': collision_adjustments\n        }\n\nclass LanguageGuidedMotionPlanner(nn.Module):\n    def __init__(self, hidden_dim=4096, action_dim=7):\n        super(LanguageGuidedMotionPlanner, self).__init__()\n\n        self.action_dim = action_dim\n\n        # Language-to-motion mapping\n        self.lang_to_motion = nn.Sequential(\n            nn.Linear(hidden_dim, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim * 10)  # 10 action steps\n        )\n\n        # Waypoint generator\n        self.waypoint_generator = nn.Sequential(\n            nn.Linear(512, 256),  # Visual + language features\n            nn.ReLU(),\n            nn.Linear(256, 50 * 3),  # 50 waypoints * 3D coordinates\n            nn.Tanh()\n        )\n\n        # Motion primitive selector\n        self.motion_primitive_selector = nn.Sequential(\n            nn.Linear(hidden_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 15)  # 15 motion primitives\n        )\n\n    def forward(self, command_features, visual_features, target_position):\n        \"\"\"\n        Generate motion plan from language command and visual context\n\n        Args:\n            command_features: Language features from command\n            visual_features: Visual features of environment\n            target_position: Target position for manipulation\n        \"\"\"\n        batch_size = command_features.size(0)\n\n        # Generate motion sequence from language\n        motion_sequence = self.lang_to_motion(command_features)\n        motion_sequence = motion_sequence.view(batch_size, 10, self.action_dim)\n\n        # Generate waypoints\n        combined_features = torch.cat([command_features, visual_features], dim=-1)\n        waypoints_flat = self.waypoint_generator(combined_features)\n        waypoints = waypoints_flat.view(batch_size, 50, 3)  # (batch, 50, 3)\n\n        # Select motion primitives\n        motion_primitives = self.motion_primitive_selector(command_features)\n\n        # Adjust trajectory to target\n        target_expanded = target_position.unsqueeze(1).expand(-1, 50, -1)  # (batch, 50, 3)\n        final_waypoints = waypoints * 0.5 + target_expanded * 0.5  # Blend with target\n\n        return {\n            'motion_sequence': motion_sequence,\n            'waypoints': final_waypoints,\n            'motion_primitives': motion_primitives,\n            'target_position': target_position\n        }\n"})}),"\n",(0,a.jsx)(e.h2,{id:"perception-action-integration",children:"Perception-Action Integration"}),"\n",(0,a.jsx)(e.h3,{id:"object-grounding-and-manipulation",children:"Object Grounding and Manipulation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# perception_action_integration.py\nimport torch\nimport torch.nn as nn\n\nclass ObjectGroundingAndManipulation(nn.Module):\n    def __init__(self, hidden_dim=4096, action_dim=7):\n        super(ObjectGroundingAndManipulation, self).__init__()\n\n        self.action_dim = action_dim\n\n        # Object detection and classification\n        self.object_detector = nn.Sequential(\n            nn.Linear(512, 256),  # Visual features\n            nn.ReLU(),\n            nn.Linear(256, 100)   # 100 object classes\n        )\n\n        # Object-language grounding\n        self.grounding_network = nn.Sequential(\n            nn.Linear(512 + 4096, 512),  # visual + language features\n            nn.ReLU(),\n            nn.Linear(512, 1),  # Grounding score\n            nn.Sigmoid()\n        )\n\n        # Grasp pose prediction\n        self.grasp_predictor = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 7),  # 7-DOF grasp pose (position + quaternion)\n        )\n\n        # Manipulation action generator\n        self.action_generator = nn.Sequential(\n            nn.Linear(512 + 4096 + 7, 512),  # visual + language + grasp\n            nn.ReLU(),\n            nn.Linear(512, action_dim * 15),  # 15 action steps\n            nn.Tanh()\n        )\n\n    def forward(self, visual_features, language_features, object_boxes):\n        \"\"\"\n        Integrate perception and action for language-guided manipulation\n\n        Args:\n            visual_features: Visual features from scene\n            language_features: Language features from command\n            object_boxes: Bounding boxes of detected objects\n        \"\"\"\n        batch_size, num_objects, vis_dim = visual_features.shape\n\n        # Detect objects\n        object_logits = self.object_detector(visual_features)  # (batch, num_objects, 100)\n        object_classes = torch.argmax(object_logits, dim=-1)  # (batch, num_objects)\n\n        # Ground language to objects\n        grounded_scores = []\n        for i in range(num_objects):\n            # Combine visual and language features for grounding\n            vis_lang_features = torch.cat([\n                visual_features[:, i, :],\n                language_features.expand(-1, vis_dim).contiguous()\n            ], dim=-1)\n            score = self.grounding_network(vis_lang_features)  # (batch, 1)\n            grounded_scores.append(score)\n\n        grounding_scores = torch.cat(grounded_scores, dim=1)  # (batch, num_objects)\n        most_likely_object = torch.argmax(grounding_scores, dim=1)  # (batch,)\n\n        # Predict grasp pose for the selected object\n        selected_object_features = visual_features[torch.arange(batch_size), most_likely_object]\n        grasp_pose = self.grasp_predictor(selected_object_features)  # (batch, 7)\n\n        # Generate manipulation actions\n        combined_features = torch.cat([\n            selected_object_features,\n            language_features,\n            grasp_pose\n        ], dim=1)\n        action_sequence = self.action_generator(combined_features)\n        action_sequence = action_sequence.view(batch_size, 15, self.action_dim)\n\n        return {\n            'object_classes': object_classes,\n            'grounding_scores': grounding_scores,\n            'selected_object': most_likely_object,\n            'grasp_pose': grasp_pose,\n            'action_sequence': action_sequence,\n            'object_boxes': object_boxes\n        }\n\nclass MultimodalManipulationController(nn.Module):\n    def __init__(self, hidden_dim=4096, action_dim=7):\n        super(MultimodalManipulationController, self).__init__()\n\n        # Visual processing\n        self.visual_processor = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.LayerNorm(256)\n        )\n\n        # Language processing\n        self.language_processor = nn.Sequential(\n            nn.Linear(hidden_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.LayerNorm(256)\n        )\n\n        # Multimodal fusion\n        self.fusion = nn.MultiheadAttention(\n            embed_dim=256,\n            num_heads=8,\n            batch_first=True\n        )\n\n        # Manipulation policy\n        self.manipulation_policy = nn.Sequential(\n            nn.Linear(256 * 2, 512),  # fused features * 2 (current + goal)\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim),\n            nn.Tanh()\n        )\n\n        # Skill sequence planner\n        self.skill_planner = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 50)  # Maximum 50 skills in sequence\n        )\n\n    def forward(self, visual_features, language_features, current_state, goal_state):\n        \"\"\"\n        Generate manipulation actions from multimodal inputs\n\n        Args:\n            visual_features: Current visual scene features\n            language_features: Language command features\n            current_state: Current robot state\n            goal_state: Target state\n        \"\"\"\n        batch_size = visual_features.size(0)\n\n        # Process visual and language features\n        vis_processed = self.visual_processor(visual_features)\n        lang_processed = self.language_processor(language_features)\n\n        # Multimodal fusion\n        fused_features, attention_weights = self.fusion(\n            vis_processed.unsqueeze(1),  # query\n            lang_processed.unsqueeze(1), # key\n            lang_processed.unsqueeze(1)  # value\n        )\n        fused_features = fused_features.squeeze(1)  # (batch, 256)\n\n        # Combine with state information\n        state_features = torch.cat([current_state, goal_state], dim=-1)\n        policy_input = torch.cat([fused_features, state_features], dim=-1)\n\n        # Generate action\n        action = self.manipulation_policy(policy_input)\n\n        # Plan skill sequence\n        skill_sequence = self.skill_planner(fused_features)\n\n        return {\n            'action': action,\n            'skill_sequence': skill_sequence,\n            'fused_features': fused_features,\n            'attention_weights': attention_weights\n        }\n"})}),"\n",(0,a.jsx)(e.h2,{id:"implementation-examples",children:"Implementation Examples"}),"\n",(0,a.jsx)(e.h3,{id:"complete-language-guided-manipulation-system",children:"Complete Language-Guided Manipulation System"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# complete_manipulation_system.py\nimport torch\nimport torch.nn as nn\n\nclass CompleteLanguageGuidedManipulation(nn.Module):\n    def __init__(self, language_model_name=\"meta-llama/Llama-2-7b-hf\"):\n        super(CompleteLanguageGuidedManipulation, self).__init__()\n\n        # Command processing\n        self.command_parser = AdvancedCommandParser()\n\n        # Skill framework\n        self.skill_executor = SkillExecutor()\n\n        # Motion planning\n        self.motion_planner = LanguageGuidedMotionPlanner()\n\n        # Perception-action integration\n        self.perception_action = ObjectGroundingAndManipulation()\n\n        # Multimodal controller\n        self.controller = MultimodalManipulationController()\n\n    def forward(self,\n                images,\n                command_text,\n                current_robot_state,\n                detected_objects=None,\n                target_location=None):\n        \"\"\"\n        Complete language-guided manipulation pipeline\n\n        Args:\n            images: Current scene images\n            command_text: Natural language command\n            current_robot_state: Current robot joint positions/velocities\n            detected_objects: Optional pre-detected objects\n            target_location: Optional target location\n        \"\"\"\n        # This is a high-level pipeline - in practice, you would have:\n        # 1. Process the language command\n        # 2. Analyze the visual scene\n        # 3. Ground language to visual objects\n        # 4. Plan manipulation actions\n        # 5. Execute the plan\n\n        # For this example, we'll return a structured manipulation plan\n        manipulation_plan = {\n            'command': command_text,\n            'parsed_command': self.parse_command(command_text),\n            'detected_objects': detected_objects,\n            'target_object': self.select_target_object(command_text, detected_objects),\n            'manipulation_action': self.determine_manipulation_action(command_text),\n            'motion_plan': self.generate_motion_plan(command_text, detected_objects),\n            'execution_status': 'planned'\n        }\n\n        return manipulation_plan\n\n    def parse_command(self, command_text):\n        \"\"\"Parse command text (simplified for this example)\"\"\"\n        # In practice, this would use the command_parser\n        return {\n            'raw_command': command_text,\n            'command_type': 'pick_and_place',  # Simplified\n            'target_object': self.extract_object(command_text),\n            'target_location': self.extract_location(command_text)\n        }\n\n    def extract_object(self, command_text):\n        \"\"\"Extract target object from command\"\"\"\n        # Simple keyword matching for demonstration\n        objects = ['block', 'cup', 'bottle', 'box', 'ball']\n        for obj in objects:\n            if obj in command_text.lower():\n                return obj\n        return 'object'\n\n    def extract_location(self, command_text):\n        \"\"\"Extract target location from command\"\"\"\n        locations = ['table', 'shelf', 'box', 'bin']\n        for loc in locations:\n            if loc in command_text.lower():\n                return loc\n        return 'location'\n\n    def select_target_object(self, command_text, detected_objects):\n        \"\"\"Select target object based on command and detection\"\"\"\n        if detected_objects:\n            # In practice, use grounding network\n            return detected_objects[0]  # First detected object\n        return None\n\n    def determine_manipulation_action(self, command_text):\n        \"\"\"Determine manipulation action from command\"\"\"\n        if 'pick' in command_text.lower() or 'grasp' in command_text.lower():\n            return 'pick'\n        elif 'place' in command_text.lower() or 'put' in command_text.lower():\n            return 'place'\n        elif 'move' in command_text.lower():\n            return 'move'\n        return 'manipulate'\n\n    def generate_motion_plan(self, command_text, detected_objects):\n        \"\"\"Generate motion plan for manipulation\"\"\"\n        return {\n            'waypoints': [],  # Planned waypoints\n            'gripper_commands': [],  # Gripper actions\n            'collision_free': True  # Collision check result\n        }\n\n# Example usage\ndef example_usage():\n    \"\"\"Example of how to use the language-guided manipulation system\"\"\"\n    # Initialize the system\n    manipulation_system = CompleteLanguageGuidedManipulation()\n\n    # Example command\n    command = \"Pick up the red block and place it on the table\"\n\n    # Example robot state (simplified)\n    robot_state = torch.randn(1, 7)  # 7-DOF robot arm\n\n    # Example detected objects (simplified)\n    detected_objects = [\n        {'class': 'block', 'position': [0.5, 0.3, 0.1], 'color': 'red'},\n        {'class': 'cup', 'position': [0.6, 0.4, 0.1], 'color': 'blue'}\n    ]\n\n    # Generate manipulation plan\n    plan = manipulation_system(\n        images=None,  # Would be actual images in practice\n        command_text=command,\n        current_robot_state=robot_state,\n        detected_objects=detected_objects\n    )\n\n    print(f\"Command: {command}\")\n    print(f\"Action: {plan['manipulation_action']}\")\n    print(f\"Target: {plan['target_object']}\")\n    print(f\"Status: {plan['execution_status']}\")\n\nif __name__ == \"__main__\":\n    example_usage()\n"})}),"\n",(0,a.jsx)(e.h3,{id:"real-world-integration-example",children:"Real-World Integration Example"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# real_world_integration.py\nimport rospy\nfrom geometry_msgs.msg import Pose, Point\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass RealWorldManipulationInterface:\n    def __init__(self):\n        # ROS publishers/subscribers\n        self.bridge = CvBridge()\n\n        # Subscribe to camera feed\n        self.image_sub = rospy.Subscriber("/camera/rgb/image_raw", Image, self.image_callback)\n\n        # Subscribe to robot state\n        self.state_sub = rospy.Subscriber("/joint_states", JointState, self.state_callback)\n\n        # Publisher for robot commands\n        self.command_pub = rospy.Publisher("/joint_group_position_controller/command", JointTrajectory, queue_size=10)\n\n        # Language-guided manipulation system\n        self.manipulation_system = CompleteLanguageGuidedManipulation()\n\n        # Current state storage\n        self.current_image = None\n        self.current_state = None\n        self.object_detector = None  # Would be initialized with actual detector\n\n    def image_callback(self, msg):\n        """Callback for camera images"""\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Store for processing\n            self.current_image = cv_image\n\n            # Run object detection if needed\n            if self.object_detector:\n                detected_objects = self.object_detector.detect(cv_image)\n                self.process_detected_objects(detected_objects)\n\n        except Exception as e:\n            rospy.logerr(f"Error processing image: {e}")\n\n    def state_callback(self, msg):\n        """Callback for robot state"""\n        # Process joint states\n        self.current_state = np.array(msg.position)\n\n    def execute_language_command(self, command_text):\n        """Execute a language command in the real world"""\n        if self.current_image is None or self.current_state is None:\n            rospy.logwarn("Waiting for image and state data...")\n            return False\n\n        # Convert current state to tensor\n        state_tensor = torch.FloatTensor(self.current_state).unsqueeze(0)\n\n        # In a real implementation, you would:\n        # 1. Process the current image through the perception pipeline\n        # 2. Run the manipulation system to generate actions\n        # 3. Execute the actions on the real robot\n\n        # For this example, we\'ll return a placeholder\n        rospy.loginfo(f"Processing command: {command_text}")\n\n        # Generate manipulation plan\n        plan = self.manipulation_system(\n            images=None,  # Would process current_image\n            command_text=command_text,\n            current_robot_state=state_tensor,\n            detected_objects=self.get_detected_objects()  # Would come from perception\n        )\n\n        # Execute the plan (simplified)\n        self.execute_manipulation_plan(plan)\n\n        return True\n\n    def get_detected_objects(self):\n        """Get currently detected objects (placeholder)"""\n        # In practice, this would come from real object detection\n        return [\n            {\'class\': \'block\', \'position\': [0.5, 0.3, 0.1], \'features\': torch.randn(512)},\n            {\'class\': \'cup\', \'position\': [0.6, 0.4, 0.1], \'features\': torch.randn(512)}\n        ]\n\n    def execute_manipulation_plan(self, plan):\n        """Execute the generated manipulation plan"""\n        rospy.loginfo(f"Executing manipulation plan: {plan[\'manipulation_action\']}")\n\n        # In practice, this would send actual commands to the robot\n        # For now, we\'ll just log the action\n        if plan[\'manipulation_action\'] == \'pick\':\n            self.execute_pick_action(plan[\'target_object\'])\n        elif plan[\'manipulation_action\'] == \'place\':\n            self.execute_place_action(plan[\'target_object\'], plan.get(\'target_location\'))\n        else:\n            rospy.logwarn(f"Unknown manipulation action: {plan[\'manipulation_action\']}")\n\n    def execute_pick_action(self, target_object):\n        """Execute pick action"""\n        rospy.loginfo(f"Picking up {target_object}")\n        # Implementation would move robot to object and grasp it\n\n    def execute_place_action(self, target_object, target_location):\n        """Execute place action"""\n        rospy.loginfo(f"Placing {target_object} at {target_location}")\n        # Implementation would move robot to location and release object\n'})}),"\n",(0,a.jsx)(e.h2,{id:"evaluation-and-performance-metrics",children:"Evaluation and Performance Metrics"}),"\n",(0,a.jsx)(e.h3,{id:"manipulation-performance-evaluation",children:"Manipulation Performance Evaluation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# manipulation_evaluation.py\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nclass ManipulationEvaluator:\n    def __init__(self):\n        self.metrics = {}\n\n    def evaluate_command_understanding(self, predicted_commands, ground_truth_commands):\n        \"\"\"Evaluate how well the system understands language commands\"\"\"\n        command_accuracy = accuracy_score(\n            [cmd['action_type'] for cmd in ground_truth_commands],\n            [cmd['action_type'] for cmd in predicted_commands]\n        )\n\n        # Calculate precision, recall, F1 for each command type\n        gt_actions = [cmd['action_type'] for cmd in ground_truth_commands]\n        pred_actions = [cmd['action_type'] for cmd in predicted_commands]\n\n        precision, recall, f1, support = precision_recall_fscore_support(\n            gt_actions, pred_actions, average='weighted'\n        )\n\n        return {\n            'command_accuracy': command_accuracy,\n            'command_precision': precision,\n            'command_recall': recall,\n            'command_f1': f1\n        }\n\n    def evaluate_object_grounding(self, predicted_groundings, ground_truth_groundings, iou_threshold=0.5):\n        \"\"\"Evaluate how well the system grounds language to objects\"\"\"\n        correct_groundings = 0\n        total_groundings = len(predicted_groundings)\n\n        for pred, gt in zip(predicted_groundings, ground_truth_groundings):\n            if self.calculate_iou(pred['bbox'], gt['bbox']) >= iou_threshold:\n                correct_groundings += 1\n\n        grounding_accuracy = correct_groundings / total_groundings if total_groundings > 0 else 0\n\n        return {\n            'grounding_accuracy': grounding_accuracy,\n            'iou_threshold': iou_threshold\n        }\n\n    def evaluate_manipulation_success(self, executed_actions, expected_outcomes):\n        \"\"\"Evaluate manipulation task success\"\"\"\n        successful_tasks = 0\n        total_tasks = len(executed_actions)\n\n        for action, expected in zip(executed_actions, expected_outcomes):\n            if self.check_task_success(action, expected):\n                successful_tasks += 1\n\n        success_rate = successful_tasks / total_tasks if total_tasks > 0 else 0\n\n        return {\n            'success_rate': success_rate,\n            'successful_tasks': successful_tasks,\n            'total_tasks': total_tasks\n        }\n\n    def check_task_success(self, action_result, expected_result):\n        \"\"\"Check if manipulation task was successful\"\"\"\n        # This would compare actual vs expected outcomes\n        # For example: object moved to correct location, grasp successful, etc.\n        return action_result.get('success', False) and expected_result.get('success', False)\n\n    def calculate_iou(self, box1, box2):\n        \"\"\"Calculate Intersection over Union\"\"\"\n        x1_inter = max(box1[0], box2[0])\n        y1_inter = max(box1[1], box2[1])\n        x2_inter = min(box1[2], box2[2])\n        y2_inter = min(box1[3], box2[3])\n\n        if x2_inter <= x1_inter or y2_inter <= y1_inter:\n            return 0.0\n\n        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)\n        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n        union_area = area1 + area2 - inter_area\n\n        return inter_area / union_area if union_area > 0 else 0.0\n\n    def generate_performance_report(self, system, test_dataset):\n        \"\"\"Generate comprehensive performance report\"\"\"\n        command_metrics = self.evaluate_command_understanding(\n            system.predict_commands(test_dataset['commands']),\n            test_dataset['ground_truth_commands']\n        )\n\n        grounding_metrics = self.evaluate_object_grounding(\n            system.predict_groundings(test_dataset['images'], test_dataset['commands']),\n            test_dataset['ground_truth_groundings']\n        )\n\n        manipulation_metrics = self.evaluate_manipulation_success(\n            system.execute_manipulation_tasks(test_dataset['manipulation_tasks']),\n            test_dataset['expected_outcomes']\n        )\n\n        report = f\"\"\"\n        Language-Guided Manipulation Performance Report\n        ==============================================\n\n        Command Understanding:\n        - Command Accuracy: {command_metrics['command_accuracy']:.3f}\n        - Command Precision: {command_metrics['command_precision']:.3f}\n        - Command Recall: {command_metrics['command_recall']:.3f}\n        - Command F1-Score: {command_metrics['command_f1']:.3f}\n\n        Object Grounding:\n        - Grounding Accuracy: {grounding_metrics['grounding_accuracy']:.3f}\n        - IoU Threshold: {grounding_metrics['iou_threshold']}\n\n        Manipulation Success:\n        - Task Success Rate: {manipulation_metrics['success_rate']:.3f}\n        - Successful Tasks: {manipulation_metrics['successful_tasks']}/{manipulation_metrics['total_tasks']}\n\n        System Assessment:\n        \"\"\"\n\n        if command_metrics['command_f1'] > 0.8:\n            report += \"- Excellent command understanding\\n\"\n        elif command_metrics['command_f1'] > 0.6:\n            report += \"- Good command understanding\\n\"\n        else:\n            report += \"- Command understanding needs improvement\\n\"\n\n        if grounding_metrics['grounding_accuracy'] > 0.7:\n            report += \"- Strong object grounding capabilities\\n\"\n        elif grounding_metrics['grounding_accuracy'] > 0.5:\n            report += \"- Adequate object grounding\\n\"\n        else:\n            report += \"- Object grounding needs improvement\\n\"\n\n        if manipulation_metrics['success_rate'] > 0.8:\n            report += \"- High manipulation success rate\\n\"\n        elif manipulation_metrics['success_rate'] > 0.6:\n            report += \"- Reasonable manipulation success\\n\"\n        else:\n            report += \"- Manipulation performance needs improvement\\n\"\n\n        return report\n"})}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Language-guided manipulation represents a significant advancement in human-robot interaction, enabling robots to understand and execute natural language commands for complex manipulation tasks. The key components include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Command Processing"}),": Understanding and parsing natural language commands"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Object Grounding"}),": Connecting language to specific visual objects"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action Planning"}),": Converting high-level commands to executable actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Skill Execution"}),": Performing manipulation tasks using learned skills"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Perception Integration"}),": Combining visual and linguistic information"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"The integration of these components creates a system capable of interpreting human instructions and executing corresponding manipulation behaviors, bridging the gap between natural language communication and robotic action."}),"\n",(0,a.jsx)(e.p,{children:"In the next section, we'll explore how to integrate all these VLA components into a complete, end-to-end trainable system."})]})}function m(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(n){const e=a.useContext(o);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:s(n.components),a.createElement(o.Provider,{value:e},n.children)}}}]);