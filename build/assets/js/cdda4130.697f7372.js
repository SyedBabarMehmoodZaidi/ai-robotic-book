"use strict";(globalThis.webpackChunkai_robotics_book=globalThis.webpackChunkai_robotics_book||[]).push([[467],{8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}},9660:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var o=t(4848),i=t(8453);const a={sidebar_position:5},s="Reinforcement Learning for Locomotion: AI-Driven Movement Control",r={id:"module-3-ai-robot-brain/reinforcement-learning-locomotion",title:"Reinforcement Learning for Locomotion: AI-Driven Movement Control",description:"Learning Objectives",source:"@site/docs/module-3-ai-robot-brain/reinforcement-learning-locomotion.md",sourceDirName:"module-3-ai-robot-brain",slug:"/module-3-ai-robot-brain/reinforcement-learning-locomotion",permalink:"/ai-robotic-book/docs/module-3-ai-robot-brain/reinforcement-learning-locomotion",draft:!1,unlisted:!1,editUrl:"https://github.com/your-username/ai-robotic-book/tree/main/docs/module-3-ai-robot-brain/reinforcement-learning-locomotion.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Nav2 Path Planning: Advanced Navigation and Autonomous Movement",permalink:"/ai-robotic-book/docs/module-3-ai-robot-brain/nav2-path-planning"},next:{title:"Module 3 Exercises: AI-Robot Brain Integration",permalink:"/ai-robotic-book/docs/module-3-ai-robot-brain/exercises"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Reinforcement Learning in Robotics",id:"introduction-to-reinforcement-learning-in-robotics",level:2},{value:"Key RL Concepts for Robotics",id:"key-rl-concepts-for-robotics",level:3},{value:"Agent-Environment Interaction",id:"agent-environment-interaction",level:4},{value:"Core RL Components",id:"core-rl-components",level:4},{value:"Isaac Gym for Robotics RL",id:"isaac-gym-for-robotics-rl",level:2},{value:"Introduction to Isaac Gym",id:"introduction-to-isaac-gym",level:3},{value:"Isaac Gym Architecture",id:"isaac-gym-architecture",level:3},{value:"Installing Isaac Gym",id:"installing-isaac-gym",level:3},{value:"Deep Reinforcement Learning Algorithms",id:"deep-reinforcement-learning-algorithms",level:2},{value:"Proximal Policy Optimization (PPO)",id:"proximal-policy-optimization-ppo",level:3},{value:"Twin Delayed DDPG (TD3) for Locomotion",id:"twin-delayed-ddpg-td3-for-locomotion",level:3},{value:"Bipedal Locomotion Training",id:"bipedal-locomotion-training",level:2},{value:"Environment Setup for Bipedal Robots",id:"environment-setup-for-bipedal-robots",level:3},{value:"Training Loop Implementation",id:"training-loop-implementation",level:3},{value:"Isaac Gym Integration for Locomotion",id:"isaac-gym-integration-for-locomotion",level:2},{value:"Isaac Gym Bipedal Environment",id:"isaac-gym-bipedal-environment",level:3},{value:"Perception-Action Integration",id:"perception-action-integration",level:2},{value:"Vision-Based Locomotion",id:"vision-based-locomotion",level:3},{value:"Policy Transfer and Deployment",id:"policy-transfer-and-deployment",level:2},{value:"Sim-to-Real Transfer",id:"sim-to-real-transfer",level:3},{value:"Performance Evaluation",id:"performance-evaluation",level:2},{value:"Locomotion Metrics",id:"locomotion-metrics",level:3},{value:"Troubleshooting RL for Locomotion",id:"troubleshooting-rl-for-locomotion",level:2},{value:"Common Issues and Solutions",id:"common-issues-and-solutions",level:3},{value:"1. Training Instability",id:"1-training-instability",level:4},{value:"2. Sim-to-Real Gap",id:"2-sim-to-real-gap",level:4},{value:"3. Reward Function Design",id:"3-reward-function-design",level:4},{value:"Exercise: Reinforcement Learning Locomotion",id:"exercise-reinforcement-learning-locomotion",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"reinforcement-learning-for-locomotion-ai-driven-movement-control",children:"Reinforcement Learning for Locomotion: AI-Driven Movement Control"}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this section, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand reinforcement learning fundamentals for robotics applications"}),"\n",(0,o.jsx)(n.li,{children:"Implement deep reinforcement learning algorithms for locomotion control"}),"\n",(0,o.jsx)(n.li,{children:"Train bipedal robots using Isaac Gym and reinforcement learning"}),"\n",(0,o.jsx)(n.li,{children:"Create perception-action loops for dynamic locomotion"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate and optimize locomotion policies for real-world deployment"}),"\n",(0,o.jsx)(n.li,{children:"Integrate learned locomotion policies with navigation and perception systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-reinforcement-learning-in-robotics",children:"Introduction to Reinforcement Learning in Robotics"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Reinforcement Learning (RL)"})," is a branch of machine learning where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. In robotics, RL has emerged as a powerful approach for learning complex behaviors, particularly locomotion, manipulation, and navigation tasks that are difficult to program using traditional methods."]}),"\n",(0,o.jsx)(n.p,{children:"Reinforcement learning is particularly well-suited for robotics because it:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Learns from trial and error in simulation"}),"\n",(0,o.jsx)(n.li,{children:"Adapts to environmental changes and disturbances"}),"\n",(0,o.jsx)(n.li,{children:"Generalizes across different terrains and conditions"}),"\n",(0,o.jsx)(n.li,{children:"Optimizes complex, multi-objective behaviors"}),"\n",(0,o.jsx)(n.li,{children:"Handles high-dimensional continuous action spaces"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"key-rl-concepts-for-robotics",children:"Key RL Concepts for Robotics"}),"\n",(0,o.jsx)(n.h4,{id:"agent-environment-interaction",children:"Agent-Environment Interaction"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    Action    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502             \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502              \u2502\n\u2502   Robot     \u2502              \u2502  Environment \u2502\n\u2502   (Agent)   \u2502 \u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502   (World)    \u2502\n\u2502             \u2502  Reward,Obs  \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,o.jsx)(n.h4,{id:"core-rl-components",children:"Core RL Components"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"State (s)"}),": Robot's current configuration (joint angles, velocities, IMU readings)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action (a)"}),": Motor commands or joint torques applied to the robot"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Reward (r)"}),": Feedback signal indicating task success or failure"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Policy (\u03c0)"}),": Strategy that maps states to actions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Value Function (V)"}),": Expected future rewards from a given state"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"isaac-gym-for-robotics-rl",children:"Isaac Gym for Robotics RL"}),"\n",(0,o.jsx)(n.h3,{id:"introduction-to-isaac-gym",children:"Introduction to Isaac Gym"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Isaac Gym"})," is NVIDIA's physics simulation environment specifically designed for reinforcement learning. It provides:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"GPU-accelerated physics simulation (hundreds of environments in parallel)"}),"\n",(0,o.jsx)(n.li,{children:"Direct integration with PyTorch for seamless RL training"}),"\n",(0,o.jsx)(n.li,{children:"Built-in RL examples for various robotic tasks"}),"\n",(0,o.jsx)(n.li,{children:"Support for contact sensors, IMUs, and other proprioceptive sensors"}),"\n",(0,o.jsx)(n.li,{children:"Domain randomization capabilities for robust policy learning"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"isaac-gym-architecture",children:"Isaac Gym Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Isaac Gym environment structure\nimport isaacgym\nfrom isaacgym import gymapi\nfrom isaacgym import gymtorch\nimport torch\n\nclass IsaacGymEnvironment:\n    def __init__(self):\n        # Initialize Isaac Gym\n        self.gym = gymapi.acquire_gym()\n\n        # Create simulation\n        self.sim = self.gym.create_sim(\n            device_id=0,\n            pipeline='gpu'\n        )\n\n        # Create viewer\n        self.viewer = self.gym.create_viewer(self.sim, gymapi.Vec3(0, 0, 1))\n\n        # Initialize environment parameters\n        self.envs = []\n        self.num_envs = 1024  # Parallel environments\n        self.env_spacing = 2.0\n"})}),"\n",(0,o.jsx)(n.h3,{id:"installing-isaac-gym",children:"Installing Isaac Gym"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Install Isaac Gym\npip install isaacgym\n\n# Verify installation\npython -c \"import isaacgym; print('Isaac Gym installed successfully')\"\n"})}),"\n",(0,o.jsx)(n.h2,{id:"deep-reinforcement-learning-algorithms",children:"Deep Reinforcement Learning Algorithms"}),"\n",(0,o.jsx)(n.h3,{id:"proximal-policy-optimization-ppo",children:"Proximal Policy Optimization (PPO)"}),"\n",(0,o.jsx)(n.p,{children:"PPO is one of the most popular algorithms for continuous control tasks like locomotion:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# ppo_locomotion_agent.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass ActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(ActorCritic, self).__init__()\n\n        # Actor network (policy)\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()  # Actions are normalized to [-1, 1]\n        )\n\n        # Critic network (value function)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        # Action standard deviation for exploration\n        self.log_std = nn.Parameter(torch.zeros(action_dim))\n\n    def forward(self, state):\n        action_mean = self.actor(state)\n        action_std = torch.exp(self.log_std)\n        value = self.critic(state)\n        return action_mean, action_std, value\n\n    def get_action(self, state):\n        action_mean, action_std, value = self.forward(state)\n        dist = torch.distributions.Normal(action_mean, action_std)\n        action = dist.sample()\n        log_prob = dist.log_prob(action).sum(dim=-1)\n        return action, log_prob, value\n\nclass PPOAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, eps_clip=0.2):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        self.actor_critic = ActorCritic(state_dim, action_dim).to(self.device)\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.MseLoss = nn.MSELoss()\n\n    def update(self, states, actions, rewards, log_probs, values, dones):\n        # Convert to tensors\n        states = torch.FloatTensor(states).to(self.device)\n        actions = torch.FloatTensor(actions).to(self.device)\n        rewards = torch.FloatTensor(rewards).to(self.device)\n        old_log_probs = torch.FloatTensor(log_probs).to(self.device)\n        old_values = torch.FloatTensor(values).to(self.device)\n        dones = torch.BoolTensor(dones).to(self.device)\n\n        # Calculate advantages\n        with torch.no_grad():\n            _, _, current_values = self.actor_critic(states)\n            advantages = rewards + self.gamma * current_values.squeeze() * (1 - dones.float()) - old_values.squeeze()\n            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        # Optimize policy\n        for _ in range(4):  # PPO epochs\n            action_means, action_stds, current_values = self.actor_critic(states)\n            dist = torch.distributions.Normal(action_means, action_stds)\n\n            new_log_probs = dist.log_prob(actions).sum(dim=-1)\n            ratios = torch.exp(new_log_probs - old_log_probs)\n\n            # PPO loss\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n            actor_loss = -torch.min(surr1, surr2).mean()\n\n            critic_loss = self.MseLoss(current_values.squeeze(), rewards + self.gamma * old_values.squeeze() * (1 - dones.float()))\n\n            total_loss = actor_loss + 0.5 * critic_loss\n\n            self.optimizer.zero_grad()\n            total_loss.backward()\n            self.optimizer.step()\n\n        return actor_loss.item(), critic_loss.item()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"twin-delayed-ddpg-td3-for-locomotion",children:"Twin Delayed DDPG (TD3) for Locomotion"}),"\n",(0,o.jsx)(n.p,{children:"TD3 is another effective algorithm for continuous control:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# td3_locomotion_agent.py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\n\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n\n        self.l1 = nn.Linear(state_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, action_dim)\n\n        self.max_action = max_action\n\n    def forward(self, state):\n        a = torch.relu(self.l1(state))\n        a = torch.relu(self.l2(a))\n        return self.max_action * torch.tanh(self.l3(a))\n\nclass Critic(nn.Module):\n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n\n        # Q1 architecture\n        self.l1 = nn.Linear(state_dim + action_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, 1)\n\n        # Q2 architecture\n        self.l4 = nn.Linear(state_dim + action_dim, 256)\n        self.l5 = nn.Linear(256, 256)\n        self.l6 = nn.Linear(256, 1)\n\n    def forward(self, state, action):\n        sa = torch.cat([state, action], 1)\n\n        q1 = torch.relu(self.l1(sa))\n        q1 = torch.relu(self.l2(q1))\n        q1 = self.l3(q1)\n\n        q2 = torch.relu(self.l4(sa))\n        q2 = torch.relu(self.l5(q2))\n        q2 = self.l6(q2)\n\n        return q1, q2\n\n    def Q1(self, state, action):\n        sa = torch.cat([state, action], 1)\n\n        q1 = torch.relu(self.l1(sa))\n        q1 = torch.relu(self.l2(q1))\n        q1 = self.l3(q1)\n\n        return q1\n\nclass TD3Agent:\n    def __init__(self, state_dim, action_dim, max_action, lr=1e-3):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n\n        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)\n        self.actor_target = Actor(state_dim, action_dim, max_action).to(self.device)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n\n        self.critic = Critic(state_dim, action_dim).to(self.device)\n        self.critic_target = Critic(state_dim, action_dim).to(self.device)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n\n        self.max_action = max_action\n        self.discount = 0.99\n        self.tau = 0.005\n        self.policy_noise = 0.2 * max_action\n        self.noise_clip = 0.5 * max_action\n        self.policy_freq = 2\n\n        self.total_it = 0\n\n    def select_action(self, state):\n        state = torch.FloatTensor(state.reshape(1, -1)).to(self.device)\n        return self.actor(state).cpu().data.numpy().flatten()\n\n    def train(self, replay_buffer, batch_size=256):\n        self.total_it += 1\n\n        # Sample replay buffer\n        state, action, next_state, reward, done = replay_buffer.sample(batch_size)\n\n        state = torch.FloatTensor(state).to(self.device)\n        action = torch.FloatTensor(action).to(self.device)\n        next_state = torch.FloatTensor(next_state).to(self.device)\n        reward = torch.FloatTensor(reward).to(self.device)\n        done = torch.FloatTensor(1 - done).to(self.device)\n\n        with torch.no_grad():\n            # Select action according to policy and add clipped noise\n            noise = torch.FloatTensor(action).data.normal_(0, self.policy_noise).to(self.device)\n            noise = noise.clamp(-self.noise_clip, self.noise_clip)\n            next_action = (self.actor_target(next_state) + noise).clamp(-self.max_action, self.max_action)\n\n            # Compute target Q-value\n            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n            target_Q = torch.min(target_Q1, target_Q2)\n            target_Q = reward + done * self.discount * target_Q\n\n        # Get current Q estimates\n        current_Q1, current_Q2 = self.critic(state, action)\n\n        # Compute critic loss\n        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n\n        # Optimize critic\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # Delayed policy updates\n        if self.total_it % self.policy_freq == 0:\n            # Compute actor loss\n            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n\n            # Optimize actor\n            self.actor_optimizer.zero_grad()\n            actor_loss.backward()\n            self.actor_optimizer.step()\n\n            # Update target networks\n            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\n            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"bipedal-locomotion-training",children:"Bipedal Locomotion Training"}),"\n",(0,o.jsx)(n.h3,{id:"environment-setup-for-bipedal-robots",children:"Environment Setup for Bipedal Robots"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# bipedal_env.py\nimport numpy as np\nimport torch\nimport gym\nfrom gym import spaces\n\nclass BipedalLocomotionEnv(gym.Env):\n    def __init__(self):\n        super(BipedalLocomotionEnv, self).__init__()\n\n        # Define action and observation spaces\n        self.action_space = spaces.Box(\n            low=-1.0, high=1.0, shape=(12,), dtype=np.float32  # 12 joint torques\n        )\n\n        # Observation space: joint positions, velocities, IMU readings, etc.\n        self.observation_space = spaces.Box(\n            low=-np.inf, high=np.inf, shape=(48,), dtype=np.float32\n        )\n\n        # Robot parameters\n        self.max_episode_steps = 1000\n        self.current_step = 0\n\n        # Reward parameters\n        self.forward_weight = 1.0\n        self.velocity_weight = 0.5\n        self.energy_weight = 0.01\n        self.balance_weight = 0.3\n\n    def reset(self):\n        # Reset robot to initial position\n        self.current_step = 0\n        self.robot_state = self.initialize_robot()\n\n        return self.get_observation()\n\n    def step(self, action):\n        # Apply action to robot\n        self.apply_action(action)\n\n        # Simulate physics\n        self.simulate_step()\n\n        # Calculate reward\n        reward = self.calculate_reward()\n\n        # Check if episode is done\n        done = self.is_episode_done()\n\n        # Update step count\n        self.current_step += 1\n        if self.current_step >= self.max_episode_steps:\n            done = True\n\n        return self.get_observation(), reward, done, {}\n\n    def get_observation(self):\n        # Return current robot state as observation\n        obs = np.concatenate([\n            self.robot_state.joint_positions,\n            self.robot_state.joint_velocities,\n            self.robot_state.imu_readings,\n            self.robot_state.contact_states\n        ])\n        return obs.astype(np.float32)\n\n    def calculate_reward(self):\n        # Forward velocity reward\n        forward_vel = self.robot_state.linear_velocity[0]\n        forward_reward = self.forward_weight * forward_vel\n\n        # Velocity tracking reward\n        target_vel = 1.0  # Target forward velocity\n        vel_reward = self.velocity_weight * max(0, 1.0 - abs(forward_vel - target_vel))\n\n        # Energy efficiency penalty\n        energy_penalty = self.energy_weight * np.sum(np.abs(self.robot_state.torques))\n\n        # Balance reward (keep torso upright)\n        torso_angle = self.robot_state.imu_readings[0]  # Roll angle\n        balance_reward = self.balance_weight * max(0, np.cos(torso_angle))\n\n        # Total reward\n        total_reward = forward_reward + vel_reward - energy_penalty + balance_reward\n\n        return total_reward\n\n    def is_episode_done(self):\n        # Check if robot has fallen\n        torso_height = self.robot_state.torso_position[2]\n        if torso_height < 0.3:  # Robot has fallen\n            return True\n\n        # Check if robot is walking backwards\n        forward_vel = self.robot_state.linear_velocity[0]\n        if forward_vel < -0.5:  # Walking backwards too fast\n            return True\n\n        return False\n\n    def apply_action(self, action):\n        # Apply torques to robot joints\n        self.robot_state.torques = action\n\n    def simulate_step(self):\n        # Simulate one physics step\n        # This would interface with the physics engine\n        pass\n\n    def initialize_robot(self):\n        # Initialize robot in standing position\n        # This would interface with the physics engine\n        pass\n"})}),"\n",(0,o.jsx)(n.h3,{id:"training-loop-implementation",children:"Training Loop Implementation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# locomotion_training.py\nimport torch\nimport numpy as np\nfrom collections import deque\nimport matplotlib.pyplot as plt\n\nclass LocomotionTrainer:\n    def __init__(self, agent, env, max_episodes=2000):\n        self.agent = agent\n        self.env = env\n        self.max_episodes = max_episodes\n\n        # Training metrics\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.episode_losses = []\n\n    def train(self):\n        for episode in range(self.max_episodes):\n            state = self.env.reset()\n            episode_reward = 0\n            episode_loss = 0\n            step_count = 0\n\n            done = False\n            while not done:\n                # Select action\n                action = self.agent.select_action(state)\n\n                # Add noise for exploration\n                noise = np.random.normal(0, 0.1, size=action.shape)\n                action = np.clip(action + noise, -1, 1)\n\n                # Take step in environment\n                next_state, reward, done, _ = self.env.step(action)\n\n                # Store transition\n                self.agent.replay_buffer.push(state, action, next_state, reward, done)\n\n                # Update agent\n                if len(self.agent.replay_buffer) > 1000:\n                    loss = self.agent.train()\n                    if loss is not None:\n                        episode_loss += loss\n\n                state = next_state\n                episode_reward += reward\n                step_count += 1\n\n            # Store episode metrics\n            self.episode_rewards.append(episode_reward)\n            self.episode_lengths.append(step_count)\n            if step_count > 0:\n                self.episode_losses.append(episode_loss / step_count)\n\n            # Log progress\n            if episode % 100 == 0:\n                avg_reward = np.mean(self.episode_rewards[-100:])\n                print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n\n            # Save best model\n            if episode % 500 == 0:\n                self.save_model(f\"locomotion_model_episode_{episode}.pth\")\n\n        # Plot training curves\n        self.plot_training_curves()\n\n    def save_model(self, filepath):\n        torch.save({\n            'actor_state_dict': self.agent.actor.state_dict(),\n            'critic_state_dict': self.agent.critic.state_dict(),\n            'episode_rewards': self.episode_rewards\n        }, filepath)\n\n    def plot_training_curves(self):\n        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 12))\n\n        # Plot rewards\n        ax1.plot(self.episode_rewards)\n        ax1.set_title('Episode Rewards')\n        ax1.set_xlabel('Episode')\n        ax1.set_ylabel('Reward')\n\n        # Plot episode lengths\n        ax2.plot(self.episode_lengths)\n        ax2.set_title('Episode Lengths')\n        ax2.set_xlabel('Episode')\n        ax2.set_ylabel('Steps')\n\n        # Plot losses\n        ax3.plot(self.episode_losses)\n        ax3.set_title('Training Loss')\n        ax3.set_xlabel('Episode')\n        ax3.set_ylabel('Loss')\n\n        plt.tight_layout()\n        plt.savefig('locomotion_training_curves.png')\n        plt.show()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"isaac-gym-integration-for-locomotion",children:"Isaac Gym Integration for Locomotion"}),"\n",(0,o.jsx)(n.h3,{id:"isaac-gym-bipedal-environment",children:"Isaac Gym Bipedal Environment"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# isaac_gym_bipedal.py\nimport isaacgym\nfrom isaacgym import gymapi, gymtorch\nfrom isaacgym.torch_utils import *\nimport torch\nimport numpy as np\n\nclass IsaacGymBipedalEnv:\n    def __init__(self, cfg):\n        self.cfg = cfg\n        self.device = self.cfg["device"]\n\n        # Initialize Isaac Gym\n        self.gym = gymapi.acquire_gym()\n        self.sim = self.gym.create_sim(\n            device_id=0,\n            pipeline=\'gpu\'\n        )\n\n        # Create viewer\n        self.viewer = self.gym.create_viewer(self.sim, gymapi.Vec3(0, 0, 1))\n\n        # Initialize environments\n        self._create_envs()\n\n        # Initialize tensors\n        self._initialize_tensors()\n\n        # RL parameters\n        self.dt = 1.0 / 60.0  # Physics update rate\n        self.max_episode_length = 1000\n\n    def _create_envs(self):\n        # Create terrain\n        plane_params = gymapi.PlaneParams()\n        plane_params.normal = gymapi.Vec3(0.0, 0.0, 1.0)\n        plane_params.static_friction = 1.0\n        plane_params.dynamic_friction = 1.0\n        plane_params.restitution = 0.0\n        self.gym.add_ground(self.sim, plane_params)\n\n        # Load bipedal robot asset\n        asset_root = "path/to/bipedal/robot"\n        asset_file = "bipedal_robot.urdf"\n\n        asset_options = gymapi.AssetOptions()\n        asset_options.fix_base_link = False\n        asset_options.collapse_fixed_joints = True\n        asset_options.replace_cylinder_with_capsule = True\n        asset_options.flip_visual_attachments = False\n        asset_options.armature = 0.01\n        asset_options.thickness = 0.001\n        asset_options.angular_damping = 0.01\n        asset_options.linear_damping = 0.01\n\n        self.bipedal_asset = self.gym.load_asset(\n            self.sim, asset_root, asset_file, asset_options\n        )\n\n        # Create environments\n        self.envs = []\n        num_envs = self.cfg["num_envs"]\n        spacing = 2.0\n\n        lower = gymapi.Vec3(-spacing, -spacing, 0.0)\n        upper = gymapi.Vec3(spacing, spacing, spacing)\n\n        for i in range(num_envs):\n            env = self.gym.create_env(self.sim, lower, upper, 1)\n\n            # Add robot to environment\n            pos = gymapi.Vec3(0.0, 0.0, 1.0)\n            pose = gymapi.Transform.from_rotation_translation(\n                gymapi.Quat.from_euler_zyx(0, 0, 0), pos\n            )\n\n            actor_handle = self.gym.create_actor(\n                env, self.bipedal_asset, pose, "bipedal", i, 0\n            )\n\n            # Configure DOF properties\n            dof_props = self.gym.get_actor_dof_properties(env, actor_handle)\n            dof_props["driveMode"].fill(gymapi.DOF_MODE_EFFORT)\n            dof_props["stiffness"].fill(0.0)\n            dof_props["damping"].fill(10.0)\n            self.gym.set_actor_dof_properties(env, actor_handle, dof_props)\n\n            self.envs.append(env)\n\n    def _initialize_tensors(self):\n        # Get gym tensors\n        self.dof_state_tensor = self.gym.acquire_dof_state_tensor(self.sim)\n        self.actor_root_tensor = self.gym.acquire_actor_root_state_tensor(self.sim)\n        self.sensor_tensor = self.gym.acquire_force_sensor_tensor(self.sim)\n        self.dof_command_tensor = self.gym.acquire_dof_command_force_tensor(self.sim)\n\n        # Wrap tensors in torch tensors\n        self.dof_state = gymtorch.wrap_tensor(self.dof_state_tensor).view(\n            self.cfg["num_envs"], -1, 2\n        )\n        self.root_states = gymtorch.wrap_tensor(self.actor_root_tensor).view(\n            self.cfg["num_envs"], -1, 13\n        )\n\n        # Initialize command tensor\n        self.dof_commands = torch.zeros(\n            self.cfg["num_envs"], self.cfg["num_dof"],\n            device=self.device, dtype=torch.float32\n        )\n\n    def get_observation(self):\n        # Extract observation from current state\n        # Joint positions and velocities\n        joint_pos = self.dof_state[:, :, 0]\n        joint_vel = self.dof_state[:, :, 1]\n\n        # Root state (position, orientation, linear velocity, angular velocity)\n        root_pos = self.root_states[:, 0, 0:3]\n        root_orn = self.root_states[:, 0, 3:7]\n        root_lin_vel = self.root_states[:, 0, 7:10]\n        root_ang_vel = self.root_states[:, 0, 10:13]\n\n        # Combine observations\n        obs = torch.cat([\n            joint_pos,\n            joint_vel,\n            root_pos,\n            root_orn,\n            root_lin_vel,\n            root_ang_vel\n        ], dim=-1)\n\n        return obs\n\n    def apply_action(self, actions):\n        # Apply actions to robot joints\n        self.gym.set_dof_actuation_force_tensor(\n            self.sim, gymtorch.unwrap_tensor(self.dof_commands)\n        )\n\n    def reset(self):\n        # Reset all environments to initial state\n        pass\n\n    def step(self, actions):\n        # Apply actions\n        self.apply_action(actions)\n\n        # Step simulation\n        self.gym.simulate(self.sim)\n        self.gym.fetch_results(self.sim, True)\n\n        # Update tensors\n        self.gym.refresh_dof_state_tensor(self.sim)\n        self.gym.refresh_actor_root_state_tensor(self.sim)\n\n        # Get observations\n        obs = self.get_observation()\n\n        # Calculate rewards\n        rewards = self.calculate_rewards()\n\n        # Check if done\n        dones = self.check_termination()\n\n        return obs, rewards, dones, {}\n\n    def calculate_rewards(self):\n        # Calculate rewards based on robot performance\n        # This is a simplified reward function\n        joint_pos = self.dof_state[:, :, 0]\n        joint_vel = self.dof_state[:, :, 1]\n\n        # Forward velocity reward\n        root_lin_vel = self.root_states[:, 0, 7:10]\n        forward_vel = root_lin_vel[:, 0]  # x-axis velocity\n        forward_reward = torch.clamp(forward_vel, 0.0, 2.0)\n\n        # Energy penalty\n        energy_penalty = torch.sum(torch.square(joint_vel), dim=1) * 0.001\n\n        # Balance reward\n        root_orn = self.root_states[:, 0, 3:7]\n        roll, pitch = get_euler_xyz(root_orn)\n        balance_reward = torch.cos(pitch) * torch.cos(roll) * 2.0\n\n        # Combined reward\n        rewards = forward_reward - energy_penalty + balance_reward\n\n        return rewards\n\n    def check_termination(self):\n        # Check if episode should terminate\n        root_pos = self.root_states[:, 0, 0:3]\n        root_orn = self.root_states[:, 0, 3:7]\n\n        # Check if robot has fallen\n        roll, pitch = get_euler_xyz(root_orn)\n        fallen = torch.abs(pitch) > 1.0  # Fallen if pitch > 60 degrees\n\n        # Check if robot is too low\n        too_low = root_pos[:, 2] < 0.3  # Fallen if z < 0.3\n\n        return torch.logical_or(fallen, too_low)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"perception-action-integration",children:"Perception-Action Integration"}),"\n",(0,o.jsx)(n.h3,{id:"vision-based-locomotion",children:"Vision-Based Locomotion"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# vision_locomotion.py\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport numpy as np\n\nclass VisionLocomotionPolicy(nn.Module):\n    def __init__(self, action_dim, visual_feature_dim=512):\n        super(VisionLocomotionPolicy, self).__init__()\n\n        # Visual encoder\n        self.visual_encoder = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(64 * 7 * 7, visual_feature_dim),\n            nn.ReLU()\n        )\n\n        # Proprioceptive encoder\n        self.proprio_encoder = nn.Sequential(\n            nn.Linear(24, 128),  # 12 joint positions + 12 joint velocities\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU()\n        )\n\n        # Combined policy\n        self.policy = nn.Sequential(\n            nn.Linear(visual_feature_dim + 128 + 6, 256),  # +6 for IMU\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim),\n            nn.Tanh()\n        )\n\n    def forward(self, visual_input, proprio_input, imu_input):\n        visual_features = self.visual_encoder(visual_input)\n        proprio_features = self.proprio_encoder(proprio_input)\n\n        combined_features = torch.cat([\n            visual_features,\n            proprio_features,\n            imu_input\n        ], dim=-1)\n\n        return self.policy(combined_features)\n\nclass VisionLocomotionAgent:\n    def __init__(self, action_dim=12):\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        self.policy = VisionLocomotionPolicy(action_dim).to(self.device)\n        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=1e-4)\n\n        # Image preprocessing\n        self.transform = transforms.Compose([\n            transforms.Resize((64, 64)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                               std=[0.229, 0.224, 0.225])\n        ])\n\n    def get_action(self, image, proprio_data, imu_data):\n        # Preprocess image\n        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n        proprio_tensor = torch.FloatTensor(proprio_data).unsqueeze(0).to(self.device)\n        imu_tensor = torch.FloatTensor(imu_data).unsqueeze(0).to(self.device)\n\n        with torch.no_grad():\n            action = self.policy(image_tensor, proprio_tensor, imu_tensor)\n\n        return action.cpu().numpy().flatten()\n\n    def train_step(self, batch_images, batch_proprio, batch_imu, batch_actions):\n        # Convert to tensors\n        images = torch.stack([self.transform(img) for img in batch_images]).to(self.device)\n        proprio = torch.FloatTensor(batch_proprio).to(self.device)\n        imu = torch.FloatTensor(batch_imu).to(self.device)\n        actions = torch.FloatTensor(batch_actions).to(self.device)\n\n        # Forward pass\n        predicted_actions = self.policy(images, proprio, imu)\n\n        # Compute loss\n        loss = torch.nn.functional.mse_loss(predicted_actions, actions)\n\n        # Backward pass\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        return loss.item()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"policy-transfer-and-deployment",children:"Policy Transfer and Deployment"}),"\n",(0,o.jsx)(n.h3,{id:"sim-to-real-transfer",children:"Sim-to-Real Transfer"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# sim_to_real_transfer.py\nimport torch\nimport numpy as np\nfrom collections import deque\n\nclass SimToRealTransfer:\n    def __init__(self, sim_policy_path, real_robot_interface):\n        # Load simulation policy\n        self.sim_policy = torch.load(sim_policy_path)\n\n        # Real robot interface\n        self.real_robot = real_robot_interface\n\n        # Domain randomization parameters\n        self.domain_params = {\n            \'friction\': [0.5, 1.5],  # Randomize friction in sim\n            \'mass\': [0.8, 1.2],      # Randomize mass in sim\n            \'com_offset\': [-0.02, 0.02]  # Randomize center of mass\n        }\n\n        # Adaptation networks\n        self.adaptation_net = self._create_adaptation_network()\n\n    def _create_adaptation_network(self):\n        """Create network to adapt sim policy to real robot"""\n        return torch.nn.Sequential(\n            torch.nn.Linear(48, 128),  # Observation space\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 12),  # Action space\n            torch.nn.Tanh()\n        )\n\n    def collect_real_data(self, num_episodes=10):\n        """Collect data from real robot for adaptation"""\n        real_data = []\n\n        for episode in range(num_episodes):\n            obs = self.real_robot.reset()\n            episode_data = []\n\n            for step in range(200):  # 200 steps per episode\n                # Get action from sim policy\n                action = self.sim_policy.get_action(obs)\n\n                # Apply action to real robot\n                next_obs, reward, done, info = self.real_robot.step(action)\n\n                # Store transition\n                episode_data.append({\n                    \'obs\': obs,\n                    \'action\': action,\n                    \'next_obs\': next_obs,\n                    \'reward\': reward\n                })\n\n                obs = next_obs\n\n                if done:\n                    break\n\n            real_data.extend(episode_data)\n\n        return real_data\n\n    def adapt_policy(self, real_data):\n        """Adapt policy using real robot data"""\n        # Train adaptation network\n        optimizer = torch.optim.Adam(self.adaptation_net.parameters(), lr=1e-3)\n\n        for epoch in range(100):\n            total_loss = 0\n\n            for transition in real_data:\n                obs_tensor = torch.FloatTensor(transition[\'obs\']).unsqueeze(0)\n                action_tensor = torch.FloatTensor(transition[\'action\']).unsqueeze(0)\n\n                predicted_action = self.adaptation_net(obs_tensor)\n                loss = torch.nn.functional.mse_loss(predicted_action, action_tensor)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n            if epoch % 20 == 0:\n                print(f"Adaptation epoch {epoch}, loss: {total_loss/len(real_data):.4f}")\n\n    def deploy_policy(self):\n        """Deploy adapted policy to real robot"""\n        def policy_function(observation):\n            # Use adapted policy for real robot\n            obs_tensor = torch.FloatTensor(observation).unsqueeze(0)\n            action = self.adaptation_net(obs_tensor)\n            return action.detach().numpy().flatten()\n\n        return policy_function\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-evaluation",children:"Performance Evaluation"}),"\n",(0,o.jsx)(n.h3,{id:"locomotion-metrics",children:"Locomotion Metrics"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# locomotion_evaluation.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\nclass LocomotionEvaluator:\n    def __init__(self):\n        self.metrics = {}\n\n    def evaluate_gait(self, joint_positions, joint_velocities, time_steps):\n        """Evaluate gait quality metrics"""\n        # Calculate gait cycle parameters\n        gait_cycle_duration = self.calculate_gait_cycle(joint_positions)\n\n        # Calculate stability metrics\n        stability = self.calculate_stability(joint_positions, joint_velocities)\n\n        # Calculate energy efficiency\n        energy_efficiency = self.calculate_energy_efficiency(joint_velocities)\n\n        # Calculate forward velocity\n        forward_velocity = self.calculate_forward_velocity(time_steps)\n\n        self.metrics.update({\n            \'gait_cycle_duration\': gait_cycle_duration,\n            \'stability\': stability,\n            \'energy_efficiency\': energy_efficiency,\n            \'forward_velocity\': forward_velocity\n        })\n\n        return self.metrics\n\n    def calculate_gait_cycle(self, joint_positions):\n        """Calculate gait cycle duration from joint position data"""\n        # Find peaks in hip joint movement to identify gait cycles\n        hip_joint_data = joint_positions[:, 0]  # Assuming hip joint is first\n\n        # Find peaks using scipy\n        peaks, _ = signal.find_peaks(hip_joint_data, height=np.std(hip_joint_data))\n\n        if len(peaks) > 1:\n            # Calculate average cycle duration\n            cycle_durations = np.diff(peaks)  # Time steps between peaks\n            avg_cycle_duration = np.mean(cycle_durations)\n            return avg_cycle_duration\n        else:\n            return 0  # Not enough cycles to calculate\n\n    def calculate_stability(self, joint_positions, joint_velocities):\n        """Calculate stability metrics"""\n        # Zero Moment Point (ZMP) stability\n        # Simplified calculation - in practice, this requires full kinematics\n        com_position = self.calculate_center_of_mass(joint_positions)\n        com_velocity = self.calculate_center_of_mass_velocity(joint_velocities)\n\n        # Calculate stability margin\n        stability_margin = np.std(com_position, axis=0)  # Lower std = more stable\n        return 1.0 / (1.0 + np.mean(stability_margin))  # Higher is better\n\n    def calculate_center_of_mass(self, joint_positions):\n        """Calculate approximate center of mass"""\n        # Simplified COM calculation\n        # In practice, use full kinematic model with link masses\n        return np.mean(joint_positions, axis=1)\n\n    def calculate_center_of_mass_velocity(self, joint_velocities):\n        """Calculate COM velocity"""\n        return np.mean(joint_velocities, axis=1)\n\n    def calculate_energy_efficiency(self, joint_velocities):\n        """Calculate energy efficiency based on joint velocities"""\n        # Energy is proportional to squared velocity\n        energy_usage = np.mean(np.sum(np.square(joint_velocities), axis=1))\n        return 1.0 / (1.0 + energy_usage)  # Higher is more efficient\n\n    def calculate_forward_velocity(self, time_steps):\n        """Calculate average forward velocity"""\n        # This would be calculated from actual robot movement\n        # For now, return a placeholder\n        return 1.0  # m/s\n\n    def plot_gait_analysis(self, joint_positions, time_steps):\n        """Plot gait analysis"""\n        fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n\n        # Plot joint positions over time\n        for i in range(min(6, joint_positions.shape[1])):  # Plot first 6 joints\n            axes[0].plot(time_steps, joint_positions[:, i], label=f\'Joint {i}\')\n        axes[0].set_title(\'Joint Positions Over Time\')\n        axes[0].set_ylabel(\'Position (rad)\')\n        axes[0].legend()\n\n        # Plot gait phases\n        axes[1].plot(time_steps, np.zeros_like(time_steps), \'k--\', alpha=0.3)\n        axes[1].set_title(\'Gait Phases\')\n        axes[1].set_ylabel(\'Phase\')\n\n        # Plot stability metrics\n        stability = self.calculate_stability_over_time(joint_positions)\n        axes[2].plot(time_steps, stability)\n        axes[2].set_title(\'Stability Over Time\')\n        axes[2].set_ylabel(\'Stability\')\n        axes[2].set_xlabel(\'Time (s)\')\n\n        plt.tight_layout()\n        plt.savefig(\'gait_analysis.png\')\n        plt.show()\n\n    def calculate_stability_over_time(self, joint_positions):\n        """Calculate stability metrics over time"""\n        stability = []\n        window_size = 10\n\n        for i in range(len(joint_positions)):\n            start_idx = max(0, i - window_size)\n            window_data = joint_positions[start_idx:i+1]\n\n            if len(window_data) > 1:\n                stability_val = 1.0 / (1.0 + np.std(window_data))\n                stability.append(stability_val)\n            else:\n                stability.append(1.0)\n\n        return np.array(stability)\n\n    def generate_evaluation_report(self):\n        """Generate comprehensive evaluation report"""\n        report = f"""\n        Locomotion Performance Evaluation Report\n        ========================================\n\n        Gait Analysis:\n        - Average gait cycle duration: {self.metrics.get(\'gait_cycle_duration\', \'N/A\')}\n        - Stability score: {self.metrics.get(\'stability\', \'N/A\'):.3f}\n        - Energy efficiency: {self.metrics.get(\'energy_efficiency\', \'N/A\'):.3f}\n        - Forward velocity: {self.metrics.get(\'forward_velocity\', \'N/A\'):.2f} m/s\n\n        Performance Assessment:\n        """\n\n        # Add qualitative assessment\n        stability = self.metrics.get(\'stability\', 0)\n        if stability > 0.8:\n            report += "- Excellent stability performance\\n"\n        elif stability > 0.6:\n            report += "- Good stability performance\\n"\n        elif stability > 0.4:\n            report += "- Adequate stability performance\\n"\n        else:\n            report += "- Poor stability performance - requires improvement\\n"\n\n        velocity = self.metrics.get(\'forward_velocity\', 0)\n        if velocity > 1.0:\n            report += "- Good forward speed performance\\n"\n        elif velocity > 0.5:\n            report += "- Adequate forward speed performance\\n"\n        else:\n            report += "- Low forward speed - may need optimization\\n"\n\n        return report\n'})}),"\n",(0,o.jsx)(n.h2,{id:"troubleshooting-rl-for-locomotion",children:"Troubleshooting RL for Locomotion"}),"\n",(0,o.jsx)(n.h3,{id:"common-issues-and-solutions",children:"Common Issues and Solutions"}),"\n",(0,o.jsx)(n.h4,{id:"1-training-instability",children:"1. Training Instability"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Solution: Implement curriculum learning\nclass CurriculumLearning:\n    def __init__(self):\n        self.difficulty_level = 0\n        self.curriculum = [\n            {'terrain': 'flat', 'speed': 0.5, 'duration': 500},\n            {'terrain': 'flat', 'speed': 1.0, 'duration': 500},\n            {'terrain': 'sloped', 'speed': 1.0, 'duration': 500},\n            {'terrain': 'rough', 'speed': 1.0, 'duration': 500}\n        ]\n\n    def get_current_task(self):\n        return self.curriculum[self.difficulty_level]\n\n    def increase_difficulty(self, success_rate):\n        if success_rate > 0.8 and self.difficulty_level < len(self.curriculum) - 1:\n            self.difficulty_level += 1\n            print(f\"Increasing difficulty to level {self.difficulty_level}\")\n"})}),"\n",(0,o.jsx)(n.h4,{id:"2-sim-to-real-gap",children:"2. Sim-to-Real Gap"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Solution: Domain randomization\ndef apply_domain_randomization(env, episode):\n    # Randomize physical parameters each episode\n    friction = np.random.uniform(0.5, 1.5)\n    mass_multiplier = np.random.uniform(0.8, 1.2)\n    com_offset = np.random.uniform(-0.02, 0.02, 3)\n\n    env.set_friction(friction)\n    env.set_mass_multiplier(mass_multiplier)\n    env.set_com_offset(com_offset)\n"})}),"\n",(0,o.jsx)(n.h4,{id:"3-reward-function-design",children:"3. Reward Function Design"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Well-designed reward function\ndef locomotion_reward(robot_state, action, prev_robot_state):\n    # Forward velocity reward (primary objective)\n    forward_vel = robot_state.linear_velocity[0]\n    forward_reward = max(0, forward_vel) * 10.0\n\n    # Energy efficiency penalty\n    energy_penalty = np.sum(np.square(action)) * 0.01\n\n    # Balance reward (keep upright)\n    roll, pitch = robot_state.imu_euler\n    balance_reward = np.cos(pitch) * np.cos(roll) * 5.0\n\n    # Smoothness penalty\n    action_diff = action - prev_robot_state.prev_action\n    smoothness_penalty = np.sum(np.square(action_diff)) * 0.1\n\n    # Avoid falling\n    height_penalty = 0\n    if robot_state.torso_height < 0.5:\n        height_penalty = -100  # Large penalty for falling\n\n    total_reward = forward_reward - energy_penalty + balance_reward - smoothness_penalty + height_penalty\n    return total_reward\n"})}),"\n",(0,o.jsx)(n.h2,{id:"exercise-reinforcement-learning-locomotion",children:"Exercise: Reinforcement Learning Locomotion"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Install Isaac Gym and set up a bipedal robot environment"}),"\n",(0,o.jsx)(n.li,{children:"Implement a PPO agent for locomotion control"}),"\n",(0,o.jsx)(n.li,{children:"Design a reward function that promotes stable forward walking"}),"\n",(0,o.jsx)(n.li,{children:"Train the agent in simulation with domain randomization"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate the learned policy's performance using gait analysis"}),"\n",(0,o.jsx)(n.li,{children:"Implement sim-to-real transfer techniques for real robot deployment"}),"\n",(0,o.jsx)(n.li,{children:"Test the policy on different terrains and conditions"}),"\n",(0,o.jsx)(n.li,{children:"Analyze the gait patterns and stability metrics"}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This exercise will give you hands-on experience with AI-driven locomotion control."}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Reinforcement learning provides a powerful approach to learning complex locomotion behaviors that are difficult to program manually. By combining Isaac Gym's parallel simulation capabilities with deep RL algorithms like PPO and TD3, robots can learn to walk, run, and navigate diverse terrains. The key to success lies in proper reward function design, curriculum learning, and sim-to-real transfer techniques that bridge the gap between simulation and reality."}),"\n",(0,o.jsx)(n.p,{children:"In the next section, we'll create exercises that integrate all the concepts from Module 3."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);